{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Build a Customer Support Question Answering Chatbot**\n",
        "**Introduction**\n",
        "\n",
        "As we witness accelerated technological progress, large language models like GPT-4 and ChatGPT have emerged as significant breakthroughs in the tech landscape. These state-of-the-art models demonstrate exceptional prowess in content generation. However, they are not without their share of challenges, such as biases and hallucinations. Despite these limitations, LLMs have the potential to bring about a transformative impact on chatbot development.\n",
        "\n",
        "Traditional, primarily intent-based chatbots have been designed to respond to specific user intents. These intents comprise a collection of sample questions and corresponding responses. For instance, a \"Restaurant Recommendations\" intent might include sample questions like \"Can you suggest a good Italian restaurant nearby?\" or \"Where can I find the best sushi in town?\" with responses such as \"You can try the Italian restaurant 'La Trattoria' nearby\" or \"The top-rated sushi place in town is 'Sushi Palace.'\"\n",
        "\n",
        "When users interact with the chatbot, their queries are matched to the most similar intent, generating the associated response. However, as LLMs continue to evolve, chatbot development is shifting toward more sophisticated and dynamic solutions capable of handling a broader range of user inquiries with greater precision and nuance.\n",
        "\n",
        "**Having a Knowledge Base**\n",
        "LLMs can significantly enhance chatbot functionality by associating broader intents with documents from a Knowledge Base (KB) instead of specific questions and answers. This approach streamlines intent management and generates more tailored responses to user inquiries.\n",
        "\n",
        "GPT3 has a maximum prompt size of around 4,000 tokens, which is substantial but insufficient for incorporating an entire knowledge base in a single prompt.\n",
        "\n",
        "Future LLMs may not have this limitation while retaining the text generation capabilities. However, for now, we need to design a solution around it.\n",
        "\n",
        "**Workflow**\n",
        "This project aims to build a chatbot that leverages GPT3 to search for answers within documents. The workflow for the experiment is explained in the following diagram."
      ],
      "metadata": {
        "id": "VYKwhb9c-6vC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we scrape some content from online articles, we split them into small chunks, compute their embeddings and store them in Deep Lake. Then, we use a user query to retrieve the most relevant chunks from Deep Lake, we put them into a prompt, which will be used to generate the final answer by the LLM.\n",
        "\n",
        "It is important to note that there is always a risk of generating hallucinations or false information when using LLMs. Although this might not be acceptable for many customers support use cases, the chatbot can still be helpful for assisting operators in drafting answers that they can double-check before sending them to the user.\n",
        "\n",
        "In the next steps, we'll explore how to manage conversations with GPT-3 and provide examples to demonstrate the effectiveness of this workflow:\n",
        "\n",
        "# **First, set up the OPENAI_API_KEY and ACTIVELOOP_TOKEN environment variables with your API keys and tokens.**\n",
        "\n",
        "As we’re going to use the SeleniumURLLoader LangChain class, and it uses the unstructured and selenium Python library, let’s install it using pip. It is recommended to install the latest version of the library. Nonetheless, please be aware that the code has been tested specifically on version 0.7.7."
      ],
      "metadata": {
        "id": "A1pAn_hE_XB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install unstructured selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mD-9vywY_zys",
        "outputId": "f25b2c94-66dd-4748-8043-f45d1c885573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unstructured\n",
            "  Downloading unstructured-0.12.4-py3-none-any.whl (1.8 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.8 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting selenium\n",
            "  Downloading selenium-4.17.2-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.10.1-py2.py3-none-any.whl (421 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dataclasses-json (from unstructured)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2024.2.7-py3-none-any.whl (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.25.2)\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff (from unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.0)\n",
            "Collecting unstructured-client>=0.15.1 (from unstructured)\n",
            "  Downloading unstructured_client-0.18.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.14.1)\n",
            "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.24.0-py3-none-any.whl (460 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.2/460.2 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.2.2)\n",
            "Requirement already satisfied: attrs>=20.1.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.6)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: charset-normalizer>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (3.3.2)\n",
            "Collecting dataclasses-json-speakeasy>=0.5.11 (from unstructured-client>=0.15.1->unstructured)\n",
            "  Downloading dataclasses_json_speakeasy-0.5.11-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpath-python>=1.0.6 (from unstructured-client>=0.15.1->unstructured)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Collecting marshmallow>=3.19.0 (from unstructured-client>=0.15.1->unstructured)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mypy-extensions>=1.0.0 (from unstructured-client>=0.15.1->unstructured)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (23.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (2.8.2)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (1.16.0)\n",
            "Collecting typing-inspect>=0.9.0 (from unstructured-client>=0.15.1->unstructured)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (4.66.2)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=ddad7d8b6265176f0b03cb62564c23d2b37ae21fd543142d876616e41fa6a8d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: filetype, rapidfuzz, python-magic, python-iso639, outcome, mypy-extensions, marshmallow, langdetect, jsonpath-python, h11, emoji, backoff, wsproto, typing-inspect, trio, trio-websocket, dataclasses-json-speakeasy, dataclasses-json, unstructured-client, selenium, unstructured\n",
            "Successfully installed backoff-2.2.1 dataclasses-json-0.6.4 dataclasses-json-speakeasy-0.5.11 emoji-2.10.1 filetype-1.2.0 h11-0.14.0 jsonpath-python-1.0.6 langdetect-1.0.9 marshmallow-3.20.2 mypy-extensions-1.0.0 outcome-1.3.0.post0 python-iso639-2024.2.7 python-magic-0.4.27 rapidfuzz-3.6.1 selenium-4.17.2 trio-0.24.0 trio-websocket-0.11.1 typing-inspect-0.9.0 unstructured-0.12.4 unstructured-client-0.18.0 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain==0.0.208 deeplake openai==0.27.8 tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lGYKw6R_z2L",
        "outputId": "97612fa1-e524-420e-cf5e-65eaf12f1adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.0.208\n",
            "  Downloading langchain-0.0.208-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deeplake\n",
            "  Downloading deeplake-3.8.20.tar.gz (585 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.8/585.8 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting openai==0.27.8\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.208)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langchainplus-sdk>=0.0.13 (from langchain==0.0.208)\n",
            "  Downloading langchainplus_sdk-0.0.20-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (2.9.0)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (1.25.2)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.208)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<2,>=1 (from langchain==0.0.208)\n",
            "  Downloading pydantic-1.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (8.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (4.66.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from deeplake) (9.4.0)\n",
            "Collecting boto3 (from deeplake)\n",
            "  Downloading boto3-1.34.44-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from deeplake) (8.1.7)\n",
            "Collecting pathos (from deeplake)\n",
            "  Downloading pathos-0.3.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting humbug>=0.3.1 (from deeplake)\n",
            "  Downloading humbug-0.3.2-py3-none-any.whl (15 kB)\n",
            "Collecting lz4 (from deeplake)\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from deeplake) (2.3.0)\n",
            "Collecting libdeeplake==0.0.101 (from deeplake)\n",
            "  Downloading libdeeplake-0.0.101-cp310-cp310-manylinux2014_x86_64.whl (16.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aioboto3>=10.4.0 (from deeplake)\n",
            "  Downloading aioboto3-12.3.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from deeplake) (1.6.0)\n",
            "Collecting dill (from libdeeplake==0.0.101->deeplake)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Collecting aiobotocore[boto3]==2.11.2 (from aioboto3>=10.4.0->deeplake)\n",
            "  Downloading aiobotocore-2.11.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.34.35,>=1.33.2 (from aiobotocore[boto3]==2.11.2->aioboto3>=10.4.0->deeplake)\n",
            "  Downloading botocore-1.34.34-py3-none-any.whl (11.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.11.2->aioboto3>=10.4.0->deeplake) (1.14.1)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore[boto3]==2.11.2->aioboto3>=10.4.0->deeplake)\n",
            "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
            "Collecting boto3 (from deeplake)\n",
            "  Downloading boto3-1.34.34-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (1.9.4)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->deeplake)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->deeplake)\n",
            "  Downloading s3transfer-0.10.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.208) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.208) (3.0.3)\n",
            "Collecting ppft>=1.7.6.8 (from pathos->deeplake)\n",
            "  Downloading ppft-1.7.6.8-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pox>=0.3.4 (from pathos->deeplake)\n",
            "  Downloading pox-0.3.4-py3-none-any.whl (29 kB)\n",
            "Collecting multiprocess>=0.70.16 (from pathos->deeplake)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.34.35,>=1.33.2->aiobotocore[boto3]==2.11.2->aioboto3>=10.4.0->deeplake) (2.8.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.35,>=1.33.2->aiobotocore[boto3]==2.11.2->aioboto3>=10.4.0->deeplake) (1.16.0)\n",
            "Building wheels for collected packages: deeplake\n",
            "  Building wheel for deeplake (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeplake: filename=deeplake-3.8.20-py3-none-any.whl size=704926 sha256=1d4b1ae964ac7ab7382da2f5d1faab3944a0146381f1d0fff7e1f58f5a98d98a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/0c/45/76d618fadd335229ef5170a4cb3cb44fff23fdf6eca13173a0\n",
            "Successfully built deeplake\n",
            "Installing collected packages: pydantic, ppft, pox, lz4, jmespath, dill, aioitertools, tiktoken, openapi-schema-pydantic, multiprocess, libdeeplake, langchainplus-sdk, humbug, dataclasses-json, botocore, s3transfer, pathos, openai, langchain, aiobotocore, boto3, aioboto3, deeplake\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.6.1\n",
            "    Uninstalling pydantic-2.6.1:\n",
            "      Successfully uninstalled pydantic-2.6.1\n",
            "  Attempting uninstall: dataclasses-json\n",
            "    Found existing installation: dataclasses-json 0.6.4\n",
            "    Uninstalling dataclasses-json-0.6.4:\n",
            "      Successfully uninstalled dataclasses-json-0.6.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aioboto3-12.3.0 aiobotocore-2.11.2 aioitertools-0.11.0 boto3-1.34.34 botocore-1.34.34 dataclasses-json-0.5.14 deeplake-3.8.20 dill-0.3.8 humbug-0.3.2 jmespath-1.0.1 langchain-0.0.208 langchainplus-sdk-0.0.20 libdeeplake-0.0.101 lz4-4.3.3 multiprocess-0.70.16 openai-0.27.8 openapi-schema-pydantic-1.2.4 pathos-0.3.2 pox-0.3.4 ppft-1.7.6.8 pydantic-1.10.14 s3transfer-0.10.0 tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SR8zY4Wa_z33",
        "outputId": "1150d932-1962-4fdb-8991-95ea194d15ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv('/content/APIKeys.env')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRd_evE8_z6o",
        "outputId": "bab59ee7-23b8-41df-9c63-575df6956cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_= load_dotenv(find_dotenv())\n",
        "\n",
        "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
        "ACTIVELOOP_TOKEN_new= os.environ['ACTIVELOOP_TOKEN_new']\n",
        "HUGGINGFACEHUB_API_TOKEN  = os.environ['HUGGINGFACEHUB_API_TOKEN']\n"
      ],
      "metadata": {
        "id": "MMTGUo79ApmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import DeepLake\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain import OpenAI\n",
        "from langchain.document_loaders import SeleniumURLLoader\n",
        "from langchain import PromptTemplate"
      ],
      "metadata": {
        "id": "KYu56-2cBMce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These libraries provide functionality for handling OpenAI embeddings, managing vector storage, splitting text, and interacting with the OpenAI API. They also enable the creation of a context-aware question-answering system, incorporating retrieval and text generation.\n",
        "\n",
        "The database for our chatbot will consist of articles regarding technical issues.\n",
        "\n",
        "Copy\n"
      ],
      "metadata": {
        "id": "rmh7oee6BUSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we'll use information from the following articles\n",
        "urls = ['https://beebom.com/what-is-nft-explained/',\n",
        "        'https://beebom.com/how-delete-spotify-account/',\n",
        "        'https://beebom.com/how-download-gif-twitter/',\n",
        "        'https://beebom.com/how-use-chatgpt-linux-terminal/',\n",
        "        'https://beebom.com/how-delete-spotify-account/',\n",
        "        'https://beebom.com/how-save-instagram-story-with-music/',\n",
        "        'https://beebom.com/how-install-pip-windows/',\n",
        "        'https://beebom.com/how-check-disk-usage-linux/']"
      ],
      "metadata": {
        "id": "jDu--J4RBXRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1: Split the documents into chunks and compute their embeddings**\n",
        "We load the documents from the provided URLs and split them into chunks using the CharacterTextSplitter with a chunk size of 1000 and no overlap:"
      ],
      "metadata": {
        "id": "zhpV_Iu9BbpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the selenium scraper to load the documents\n",
        "loader = SeleniumURLLoader(urls=urls)\n",
        "docs_not_splitted = loader.load()\n",
        "\n",
        "# we split the documents into smaller chunks\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(docs_not_splitted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkiambKnBdgn",
        "outputId": "233bd828-611b-4710-dc02-8659508297bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1226, which is longer than the specified 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we compute the embeddings using OpenAIEmbeddings and store them in a Deep Lake vector store on the cloud. In an ideal production scenario, we could upload a whole website or course lesson on a Deep Lake dataset, allowing for search among even thousands or millions of documents. As we are using a cloud serverless Deep Lake dataset, applications running on different locations can easily access the same centralized dataset without the need of deploying a vector store on a custom machine.\n",
        "\n",
        "Let’s now modify the following code by adding your Activeloop organization ID. It worth noting that the org id is your username by default."
      ],
      "metadata": {
        "id": "sceUIsQGBpzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before executing the following code, make sure to have\n",
        "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# create Deep Lake dataset\n",
        "# TODO: use your organization id here. (by default, org id is your username)\n",
        "my_activeloop_org_id = \"mubisain\"\n",
        "my_activeloop_dataset_name = \"langchain_course_customer_support\"\n",
        "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
        "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings,token=ACTIVELOOP_TOKEN_new)\n",
        "\n",
        "# add documents to our Deep Lake dataset\n",
        "db.add_documents(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6miPRheBsti",
        "outputId": "5a92a256-6062-42e8-cfbf-125997f84db7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Deep Lake dataset has been successfully created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating 149 embeddings in 1 batches of size 149:: 100%|██████████| 1/1 [00:04<00:00,  4.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='hub://mubisain/langchain_course_customer_support', tensors=['text', 'metadata', 'embedding', 'id'])\n",
            "\n",
            "  tensor      htype       shape      dtype  compression\n",
            "  -------    -------     -------    -------  ------- \n",
            "   text       text      (149, 1)      str     None   \n",
            " metadata     json      (149, 1)      str     None   \n",
            " embedding  embedding  (149, 1536)  float32   None   \n",
            "    id        text      (149, 1)      str     None   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0673e9fe-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673eb98-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673ec2e-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673ecb0-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673ed3c-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673edaa-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673ee18-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673ee86-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673eef4-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673ef62-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673efd0-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673f048-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673f14c-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673f1ce-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673f23c-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673f2b4-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673f336-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673f3a4-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673f412-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673f480-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673f4ee-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673f566-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673f5de-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673f656-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673f6c4-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673f732-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673f7a0-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673f818-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673f886-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673f908-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673f98a-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673f9f8-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673fa66-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673fade-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673fb4c-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673fbba-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673fc3c-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673fcaa-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673fd22-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673fd90-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673fdfe-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673fe76-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673fee4-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673ff52-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0673ffc0-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0674002e-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '067400a6-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06740114-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06740182-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '067401f0-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06740272-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '067402ea-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06740358-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '067403d0-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0674043e-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '067404ac-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0674051a-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0674057e-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '067405f6-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0674066e-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '067406dc-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0674074a-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '067407b8-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06740826-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0674089e-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0674090c-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0674097a-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '067409f2-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06740a60-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06740ac4-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06740b3c-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06740ba0-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06740c18-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06740cc2-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06740d3a-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06740db2-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06740e20-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06740ea2-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06740f10-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06740f7e-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06740ff6-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06741064-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0674112c-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0674119a-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06741212-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06741280-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '067412ee-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0674135c-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '067413d4-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06741438-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '067414b0-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0674155a-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '067415d2-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0674164a-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '067416c2-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06741726-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06741794-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06741866-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '067418de-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0674194c-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '067419b0-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06741a50-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06741abe-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06741b5e-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06741c08-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06741cb2-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06741d52-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06741e06-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06741e92-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06741f0a-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06741f82-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06741ffa-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06742072-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '067420e0-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06742162-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '067421d0-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0674223e-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '067422ac-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0674231a-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06742388-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06742446-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '067424b4-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06742522-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06742590-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '067425fe-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '0674266c-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '067426da-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06742748-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '067427c0-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06742838-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '067428a6-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06742914-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06742982-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '067429e6-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06742a5e-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06742acc-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06742b3a-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06742b9e-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06742c0c-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06742cb6-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06742d24-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06742d92-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06742e00-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06742e6e-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06742ee6-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06742f54-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06742fc2-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06743030-ce7a-11ee-92e0-0242ac1c000c',\n",
              " '06743094-ce7a-11ee-92e0-0242ac1c000c']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To retrieve the most similar chunks to a given query, we can use the similarity_search method of the Deep Lake vector store:"
      ],
      "metadata": {
        "id": "mOD8vwhoB2Hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see the top relevant documents to a specific query\n",
        "query = \"how to check disk usage in linux?\"\n",
        "docs = db.similarity_search(query)\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsU5KKTLB4Ni",
        "outputId": "4fd37a0a-6b52-4313-8df5-34cdfb7d808f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check Disk Usage Using Gnome Disk Tool\n",
            "\n",
            "Check Disk Usage Using Disk Usage Analyzer Tool\n",
            "\n",
            "Cleanup Disk using Disk Usage Analyzer\n",
            "\n",
            "Check Disk Space Using the df Command\n",
            "\n",
            "In Linux, there are many commands to check disk usage, the most common being the df command. The df stands for “Disk Filesystem” in the command, which is a handy way to check the current disk usage and the available disk space in Linux. The syntax for the df command in Linux is as follows:\n",
            "\n",
            "df <options> <file_system>\n",
            "\n",
            "The options to use with the df command are:\n",
            "\n",
            "Options Description -a Show information about all file systems including pseudo, duplicate and inaccessible file systems -h Display the sizes in human-readable format i.e in powers of 1024 -t Display the disk usage of only the file system of a particular type -x Display the disk usage excluding a particular file type\n",
            "\n",
            "Display Disk Usage in Human Readable Format\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2: Craft a prompt for GPT-3 using the suggested strategies**\n",
        "We will create a prompt template that incorporates role-prompting, relevant Knowledge Base information, and the user's question:"
      ],
      "metadata": {
        "id": "Q2HcHTlYB_2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's write a prompt for a customer support chatbot that\n",
        "# answer questions using information extracted from our db\n",
        "template = \"\"\"You are an exceptional customer support chatbot that gently answer questions.\n",
        "\n",
        "You know the following context information.\n",
        "\n",
        "{chunks_formatted}\n",
        "\n",
        "Answer to the following question from a customer. Use only information from the previous context information. Do not invent stuff.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"chunks_formatted\", \"query\"],\n",
        "    template=template,\n",
        ")"
      ],
      "metadata": {
        "id": "pMTM4U-CB-8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The template sets the chatbot's persona as an exceptional customer support chatbot.** : The template takes two input variables: chunks_formatted, which consists of the pre-formatted chunks from articles, and query, representing the customer's question. The objective is to generate an accurate answer using only the provided chunks without creating any false or invented information.\n",
        "\n"
      ],
      "metadata": {
        "id": "BtmFcMu9CJ7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3: Utilize the GPT3 model with a temperature of 0 for text generation**\n",
        "To generate a response, we first retrieve the top-k (e.g., top-3) chunks most similar to the user query, format the prompt, and send the formatted prompt to the GPT3 model with a temperature of 0.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8yiYEzS0CMD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The full pipeline**"
      ],
      "metadata": {
        "id": "g1_rbB19CbZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the full pipeline\n",
        "\n",
        "# user question\n",
        "query = \"How to check disk usage in linux?\"\n",
        "\n",
        "# retrieve relevant chunks\n",
        "docs = db.similarity_search(query)\n",
        "retrieved_chunks = [doc.page_content for doc in docs]\n",
        "\n",
        "# format the prompt\n",
        "chunks_formatted = \"\\n\\n\".join(retrieved_chunks)\n",
        "prompt_formatted = prompt.format(chunks_formatted=chunks_formatted, query=query)\n",
        "\n",
        "# generate answer\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "answer = llm(prompt_formatted)\n",
        "(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "oD5nFHRXCZtX",
        "outputId": "ba9bdc1b-1094-48da-8f61-b36321c5888f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' There are several ways to check disk usage in Linux. One of the most common methods is by using the df command. You can also use GUI tools like the Gnome Disk Tool or the Disk Usage Analyzer Tool to easily monitor disk usage. These tools provide a visual representation of the disk occupancy and allow you to delete unnecessary files or folders to free up space.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Issues with Generating Answers using GPT-3**\n",
        "In the previous example, the chatbot generally performs well. However, there are certain situations where it could fail.\n",
        "\n",
        "Suppose we ask, \"Is the Linux distribution free?\" and provide GPT-3 with a document about kernel features as context. It might generate an answer like \"Yes, the Linux distribution is free to download and use,\" even if such information is not present in the context document. Producing false information is highly undesirable for customer service chatbots!\n",
        "\n",
        "**GPT-3 is less likely to generate false information when the answer to the user's question is contained within the context.** Since user questions are often brief and ambiguous, we cannot always rely on the semantic search step to retrieve the correct document. Thus, there is always a risk of generating false information.\n",
        "\n",
        "Conclusion\n",
        "GPT-3 is highly effective in creating conversational chatbots capable of answering specific questions based on the contextual information provided in the prompt. **However, it can be challenging to ensure that the model generates answers solely based on the context, as it has a tendency to hallucinate (i.e., generate new, potentially false information)**. The severity of generating false information varies depending on the use case.\n",
        "\n",
        "To conclude, we implemented a context-aware question-answering system using LangChain, following the provided code and strategies. The process involved splitting documents into chunks, computing their embeddings, implementing a retriever to find similar chunks, crafting a prompt for GPT-3, and using the GPT3 model for text generation. This approach demonstrates the potential of leveraging GPT-3 to create powerful and contextually accurate chatbots while also highlighting the need to be cautious about the possibility of generating false information."
      ],
      "metadata": {
        "id": "lzbR4xmeDAPZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "→ ConversationBufferMemory, which is the most straightforward, then\n",
        "\n",
        "→ ConversationBufferWindowMemory, which maintains a memory window that keeps a limited number of past interactions based on the specified window size.\n",
        "\n",
        "→ And the most complex variant, ConversationSummaryMemory that holds a summary of previous converations."
      ],
      "metadata": {
        "id": "oZoKA0ItVuDa"
      }
    }
  ]
}