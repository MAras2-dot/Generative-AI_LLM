{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-index==0.9.14.post3 deeplake==3.8.8 openai==1.3.8 cohere==4.37"
      ],
      "metadata": {
        "id": "oLsz4honE_jq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "199244e1-b348-40dc-ce51-071aeff51030"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m943.5/943.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m577.4/577.4 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.5/221.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for deeplake (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "zrVTLvWC-nOQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b466702-e52e-470e-9048-9cb3d476b7c2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv('/content/APIKeys.env')"
      ],
      "metadata": {
        "id": "i0WJIPPk-nQs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "109d0681-8061-43f9-9c3e-24e12c9ec223"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_= load_dotenv(find_dotenv())\n",
        "\n",
        "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
        "ACTIVELOOP_TOKEN = os.environ['ACTIVELOOP_TOKEN_new']"
      ],
      "metadata": {
        "id": "8qf1NUWs-nS9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token= ACTIVELOOP_TOKEN"
      ],
      "metadata": {
        "id": "v2eaBvXS_qRz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Indexes"
      ],
      "metadata": {
        "id": "e4xCp6-mN3xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p 'data/1k/'\n",
        "!wget 'https://github.com/idontcalculate/data-repo/blob/main/machine_to_end_war.txt' -O './data/1k/tesla.txt'\n",
        "!wget 'https://github.com/idontcalculate/data-repo/blob/main/prodigal_chapter10.txt' -O './data/1k/web.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mE86KzdvOPgX",
        "outputId": "6d935ded-6fa8-4c9e-b0e9-11cf5eb5c8da"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-21 18:46:54--  https://github.com/idontcalculate/data-repo/blob/main/machine_to_end_war.txt\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18491 (18K) [text/plain]\n",
            "Saving to: ‘./data/1k/tesla.txt’\n",
            "\n",
            "\r./data/1k/tesla.txt   0%[                    ]       0  --.-KB/s               \r./data/1k/tesla.txt 100%[===================>]  18.06K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-02-21 18:46:54 (1.46 MB/s) - ‘./data/1k/tesla.txt’ saved [18491/18491]\n",
            "\n",
            "--2024-02-21 18:46:54--  https://github.com/idontcalculate/data-repo/blob/main/prodigal_chapter10.txt\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28083 (27K) [text/plain]\n",
            "Saving to: ‘./data/1k/web.txt’\n",
            "\n",
            "./data/1k/web.txt   100%[===================>]  27.42K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-02-21 18:46:54 (2.16 MB/s) - ‘./data/1k/web.txt’ saved [28083/28083]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From VectorStore"
      ],
      "metadata": {
        "id": "xAXcBI5COHWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import SimpleDirectoryReader\n",
        "\n",
        "tesla_docs = SimpleDirectoryReader( input_files=[\"/content/data/1k/tesla.txt\"] ).load_data()"
      ],
      "metadata": {
        "id": "ufXfJwfcORAa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9027387d-604d-46ee-d5d6-cbbf1247ac49"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.8.20) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.vector_stores import DeepLakeVectorStore\n",
        "\n",
        "my_activeloop_org_id = \"mubisain\"\n",
        "my_activeloop_dataset_name = \"LlamaIndex_tesla_predictions\"\n",
        "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
        "\n",
        "# Create an index over the documnts\n",
        "vector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=False, token= token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--NnpT4UOQbv",
        "outputId": "06e80c64-1288-4358-cb02-cda0ea623d7d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Deep Lake dataset has been successfully created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.storage.storage_context import StorageContext\n",
        "\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
      ],
      "metadata": {
        "id": "loC5XGShPtf_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex\n",
        "\n",
        "tesla_index = VectorStoreIndex.from_documents(tesla_docs, storage_context=storage_context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pPld5m3Ptco",
        "outputId": "d7c5bdb4-98d3-40ef-c670-5f9bd478b5b3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading data to deeplake dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00,  7.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='hub://mubisain/LlamaIndex_tesla_predictions', tensors=['text', 'metadata', 'embedding', 'id'])\n",
            "\n",
            "  tensor      htype      shape     dtype  compression\n",
            "  -------    -------    -------   -------  ------- \n",
            "   text       text      (5, 1)      str     None   \n",
            " metadata     json      (5, 1)      str     None   \n",
            " embedding  embedding  (5, 1536)  float32   None   \n",
            "    id        text      (5, 1)      str     None   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From Local Index"
      ],
      "metadata": {
        "id": "GIQLCDvuQPPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "webtext_docs = SimpleDirectoryReader( input_files=[\"/content/data/1k/web.txt\"] ).load_data()"
      ],
      "metadata": {
        "id": "7lPWnGE_PtaR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  # Try to load the index if it is already calculated\n",
        "  storage_context = StorageContext.from_defaults( persist_dir=\"/content/storage/webtext\" )\n",
        "  webtext_index = load_index_from_storage(storage_context)\n",
        "  print(\"Loaded the pre-computed index.\")\n",
        "except:\n",
        "  # Otherwise, generate the indexes\n",
        "  webtext_index = VectorStoreIndex.from_documents(webtext_docs)\n",
        "  webtext_index.storage_context.persist(persist_dir=\"/content/storage/webtext\")\n",
        "  print(\"Generated the index.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1ieq9i8QUa1",
        "outputId": "2eaa7dc5-ae1d-4418-b5d4-d94fc7cd093d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated the index.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Query Enginges"
      ],
      "metadata": {
        "id": "dF3KH4bHRU9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tesla_engine = tesla_index.as_query_engine(similarity_top_k=3)\n",
        "webtext_engine = webtext_index.as_query_engine(similarity_top_k=3)"
      ],
      "metadata": {
        "id": "MR4BXAVqQnV8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the Tools"
      ],
      "metadata": {
        "id": "YFbYTky1Rgu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
        "\n",
        "query_engine_tools = [\n",
        "    QueryEngineTool(\n",
        "        query_engine=tesla_engine,\n",
        "        metadata=ToolMetadata(\n",
        "            name=\"tesla_1k\",\n",
        "            description=(\n",
        "                \"Provides information about Tesla's statements that refers to future times and predictions. \"\n",
        "                \"Use a detailed plain text question as input to the tool.\"\n",
        "            ),\n",
        "        ),\n",
        "    ),\n",
        "    QueryEngineTool(\n",
        "        query_engine=webtext_engine,\n",
        "        metadata=ToolMetadata(\n",
        "            name=\"webtext_1k\",\n",
        "            description=(\n",
        "                \"Provides information about tesla's life and biographical data. \"\n",
        "                \"Use a detailed plain text question as input to the tool.\"\n",
        "            ),\n",
        "        ),\n",
        "    ),\n",
        "]"
      ],
      "metadata": {
        "id": "vMptZI0uPtVU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Agent"
      ],
      "metadata": {
        "id": "0CU3MaJ4Rt4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.agent import OpenAIAgent\n",
        "agent = OpenAIAgent.from_tools(query_engine_tools, verbose=True)"
      ],
      "metadata": {
        "id": "RdBtZi50PtSe"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.chat_repl()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-0kHgKCRiq0",
        "outputId": "f9aa7c97-0e3d-4e8f-bf49-134557956929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Entering Chat REPL =====\n",
            "Type \"exit\" to exit.\n",
            "\n",
            "Human: What is Tesla's latest model\n",
            "STARTING TURN 1\n",
            "---------------\n",
            "\n",
            "Assistant: Tesla's latest model is the Tesla Model S Plaid. It is an electric sedan that offers impressive performance and range.\n",
            "\n",
            "Human: How much is the cost?\n",
            "STARTING TURN 1\n",
            "---------------\n",
            "\n",
            "=== Calling Function ===\n",
            "Calling function: webtext_1k with args: {\n",
            "  \"input\": \"What is the cost of the Tesla Model S Plaid?\"\n",
            "}\n",
            "Got output: The cost of the Tesla Model S Plaid is around $130,000.\n",
            "========================\n",
            "\n",
            "STARTING TURN 2\n",
            "---------------\n",
            "\n",
            "Assistant: The cost of the Tesla Model S Plaid is around $130,000.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents with Tools"
      ],
      "metadata": {
        "id": "AGakE3sO1F_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.tools import FunctionTool\n",
        "\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two integers and returns the result integer\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "multiply_tool = FunctionTool.from_defaults(fn=multiply, name=\"multiply\")\n",
        "add_tool = FunctionTool.from_defaults(fn=add, name=\"add\")\n",
        "\n",
        "all_tools = [multiply_tool, add_tool]"
      ],
      "metadata": {
        "id": "xwTwYSSjRihM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex\n",
        "from llama_index.objects import ObjectIndex, SimpleToolNodeMapping\n",
        "\n",
        "tool_mapping = SimpleToolNodeMapping.from_objects(all_tools)\n",
        "obj_index = ObjectIndex.from_objects(\n",
        "    all_tools,\n",
        "    tool_mapping,\n",
        "    VectorStoreIndex,\n",
        ")"
      ],
      "metadata": {
        "id": "gRNaPGDyRieS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.agent import FnRetrieverOpenAIAgent\n",
        "\n",
        "agent = FnRetrieverOpenAIAgent.from_retriever(\n",
        "    obj_index.as_retriever(), verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "GNHxglUDKJ5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.chat(\"What's 12 multiplied by 22? Make sure to use Tools\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSCd47OB2ZIN",
        "outputId": "665317f3-6710-4a72-f940-96fd50800ef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STARTING TURN 1\n",
            "---------------\n",
            "\n",
            "=== Calling Function ===\n",
            "Calling function: multiply with args: {\n",
            "  \"a\": 12,\n",
            "  \"b\": 22\n",
            "}\n",
            "Got output: 264\n",
            "========================\n",
            "\n",
            "STARTING TURN 2\n",
            "---------------\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentChatResponse(response='12 multiplied by 22 is 264.', sources=[ToolOutput(content='264', tool_name='multiply', raw_input={'args': (), 'kwargs': {'a': 12, 'b': 22}}, raw_output=264)], source_nodes=[])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.chat( \"What is 5 + 2?\", tool_choice=\"add\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btL4lvUy2ZBK",
        "outputId": "6b36a503-321e-489f-8101-e20890367ddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STARTING TURN 1\n",
            "---------------\n",
            "\n",
            "=== Calling Function ===\n",
            "Calling function: add with args: {\n",
            "  \"a\": 5,\n",
            "  \"b\": 2\n",
            "}\n",
            "Got output: 7\n",
            "========================\n",
            "\n",
            "STARTING TURN 2\n",
            "---------------\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentChatResponse(response='5 + 2 is equal to 7.', sources=[ToolOutput(content='7', tool_name='add', raw_input={'args': (), 'kwargs': {'a': 5, 'b': 2}}, raw_output=7)], source_nodes=[])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GHfXq66zEZ7B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}