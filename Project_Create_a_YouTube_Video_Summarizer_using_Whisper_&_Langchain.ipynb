{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Create a YouTube Video Summarizer Using Whisper and LangChain**\n",
        "\n",
        "We previously explored the powerful feature of LangChain called chains, which allow for the creation of an end-to-end pipeline for using language models. We learned how chains combine multiple components such as models, prompts, memory, parsing output, and debugging to provide a user-friendly interface. We also discussed the process of designing custom pipelines by inheriting the Chain class and explored the LLMChain as a simple example. This lesson served as a foundation for future lessons, where we will apply these concepts to a hands-on project of summarizing a YouTube video.\n",
        "\n",
        "During this lesson, we delved into the challenge of summarizing YouTube videos efficiently in the context of the digital age. It will introduce two cutting-edge tools, Whisper and LangChain, that can help tackle this issue. **We will discuss the strategies of \"stuff,\" \"map-reduce,\" and \"refine\" for handling large amounts of text and extracting valuable information.** It is possible to effectively extract key takeaways from videos by leveraging Whisper to transcribe YouTube audio files and utilizing LangChain's summarization techniques, including stuff, refine, and map_reduce. We also highlighted the customizability of LangChain, allowing personalized prompts, multilingual summaries, and storage of URLs in a Deep Lake vector store. By implementing these advanced tools, you can save time, enhance knowledge retention, and improve your understanding of various topics. Enjoy the tailored experience of data storage and summarization with LangChain and Whisper.\n",
        "\n",
        "The following diagram explains what we are going to do in this project."
      ],
      "metadata": {
        "id": "GDOhF23_01__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  First, we download the youtube video we are interested in and transcribe it using Whisper. Then, we’ll proceed by creating summaries using two different approaches:\n",
        "*   First we use an existing summarization chain to generate the final summary, which automatically manages embeddings and prompts.\n",
        "\n",
        "*   Then, we use another approach more step-by-step to generate a final summary formatted in bullet points, consisting in splitting the transcription into chunks, computing their embeddings, and preparing ad-hoc prompts.\n",
        "\n",
        "\n",
        "# **Introduction**\n",
        "In the digital era, the abundance of information can be overwhelming, and we often find ourselves scrambling to consume as much content as possible within our limited time. YouTube is a treasure trove of knowledge and entertainment, but it can be challenging to sift through long videos to extract the key takeaways. Worry not, as we've got your back! In this lesson, we will unveil a powerful solution to help you efficiently summarize YouTube videos using two cutting-edge tools: Whisper and LangChain."
      ],
      "metadata": {
        "id": "AhZhP55Z04_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will guide you through the process of downloading a YouTube audio file, transcribing it using Whisper, and then summarizing the transcribed text with LangChain's innovative **stuff, refine, and map_reduce techniques.**\n",
        "\n",
        "# **Workflow:**\n",
        "**1. Download the YouTube audio file.**\n",
        "**2. Transcribe the audio using Whisper.**\n",
        "**3. Summarize the transcribed text using LangChain with three different approaches: stuff, refine, and map_reduce.**\n",
        "**4. Adding multiple URLs to DeepLake database, and retrieving information.**\n",
        "\n",
        "## Installations:\n",
        "\n",
        "Remember to install the required packages with the following command: pip install langchain==0.1.4 deeplake openai==1.10.0 tiktoken. Additionally, install also the yt_dlp and openai-whisper packages, which have been tested in this lesson with versions  2023.6.21 and 20230314, respectively."
      ],
      "metadata": {
        "id": "q2Tg_TQe07Nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q yt_dlp\n",
        "!pip install -q git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "id": "Q4Lf3gb0094M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we must install the ffmpeg application, which is one of the requirements for the yt_dlp package. This application is installed on Google Colab instances by default. The following commands show the installation process on Mac and Ubuntu operating systems."
      ],
      "metadata": {
        "id": "l67AwH_v0-mM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MacOS (requires https://brew.sh/)\n",
        "#brew install ffmpeg\n",
        "\n",
        "# Ubuntu\n",
        "#sudo apt install ffmpegCopy\n"
      ],
      "metadata": {
        "id": "vgGxj7az1BWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can read the following article if you're working on an operating system that hasn't been mentioned earlier (like Windows). It contains comprehensive, step-by-step instructions on \"How to install ffmpeg.”\n",
        "\n",
        "Next step is to add the API key for OpenAI and Deep Lake services in the environment variables. You can either use the load_dotenv function to read the values from a .env file, or by running the following code. Remember that the API keys must remain private since anyone with this information can access these services on your behalf."
      ],
      "metadata": {
        "id": "SSXDGzDZ1Fyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = \"<OPENAI_API_KEY>\"\n",
        "os.environ['ACTIVELOOP_TOKEN'] = \"<ACTIVELOOP_TOKEN>\""
      ],
      "metadata": {
        "id": "MrHBCSs01FRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this experiment, we have selected a video featuring Yann LeCun, a distinguished computer scientist and AI researcher. In this engaging discussion, LeCun delves into the challenges posed by large language models.\n",
        "\n",
        "The download_mp4_from_youtube() function will download the best quality mp4 video file from any YouTube link and save it to the specified path and filename. We just need to copy/paste the selected video’s URL and pass it to mentioned function."
      ],
      "metadata": {
        "id": "zUAdotGD1KDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yt_dlp\n",
        "\n",
        "def download_mp4_from_youtube(url):\n",
        "    # Set the options for the download\n",
        "    filename = 'lecuninterview.mp4'\n",
        "    ydl_opts = {\n",
        "        'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]',\n",
        "        'outtmpl': filename,\n",
        "        'quiet': True,\n",
        "    }\n",
        "\n",
        "    # Download the video file\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        result = ydl.extract_info(url, download=True)\n",
        "\n",
        "url = \"https://www.youtube.com/watch?v=mBjPyte2ZZo\"\n",
        "download_mp4_from_youtube(url)"
      ],
      "metadata": {
        "id": "aIbrM0Vx1MZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it’s time for Whisper!\n",
        "\n",
        "Whisper is a cutting-edge, automatic speech recognition system developed by OpenAI. Boasting state-of-the-art capabilities, Whisper has been trained on an impressive 680,000 hours of multilingual and multitasking supervised data sourced from the web.  This vast and varied dataset enhances the system's robustness, enabling it to handle accents, background noise, and technical language easily. OpenAI has released the models and codes to provide a solid foundation for creating valuable applications harnessing the power of speech recognition.\n",
        "\n",
        "The whisper package that we installed earlier provides the .load_model() method to download the model and transcribe a video file. Multiple different models are available: tiny, base, small, medium, and large. Each one of them has tradeoffs between accuracy and speed. We will use the 'base' model for this tutorial."
      ],
      "metadata": {
        "id": "DaaU6HB51O7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"base\")\n",
        "result = model.transcribe(\"lecuninterview.mp4\")\n",
        "print(result['text'])"
      ],
      "metadata": {
        "id": "VrxrhcwS1SDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open ('text.txt', 'w') as file:\n",
        "    file.write(result['text'])"
      ],
      "metadata": {
        "id": "Hnm9-CSR1UxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Summarization with LangChain**\n",
        "We first import the necessary classes and utilities from the LangChain library."
      ],
      "metadata": {
        "id": "k6mP2sEh1XvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rom langchain import OpenAI, LLMChain\n",
        "from langchain.chains.mapreduce import MapReduceChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0)"
      ],
      "metadata": {
        "id": "XuLS2DD01XS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This imports essential components from the LangChain library for efficient text summarization and initializes an instance of OpenAI's large language model with a temperature setting of 0. The key elements include classes for handling large texts, optimization, prompt construction, and summarization techniques.\n",
        "\n",
        "This code creates an instance of the RecursiveCharacterTextSplitter\n",
        " class, which is responsible for splitting input text into smaller chunks.\n",
        "\n",
        "Copy\n"
      ],
      "metadata": {
        "id": "3s6Yy1o51ea5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"]\n",
        ")Copy\n"
      ],
      "metadata": {
        "id": "3HA1gkoq1kN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is configured with a chunk_size of 1000 characters, no chunk_overlap, and uses spaces, commas, and newline characters as separators. This ensures that the input text is broken down into manageable pieces, allowing for efficient processing by the language model.\n",
        "\n",
        "We’ll open the text file we’ve saved previously and split the transcripts using .split_text() method."
      ],
      "metadata": {
        "id": "L9zoKkyw1q4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document\n",
        "\n",
        "with open('text.txt') as f:\n",
        "    text = f.read()\n",
        "\n",
        "texts = text_splitter.split_text(text)\n",
        "docs = [Document(page_content=t) for t in texts[:4]]"
      ],
      "metadata": {
        "id": "vWdw16241tBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each Document object is initialized with the content of a chunk from the texts list. The [:4] slice notation indicates that only the first four chunks will be used to create the Document objects."
      ],
      "metadata": {
        "id": "6WZatB401u86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "import textwrap\n",
        "\n",
        "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "\n",
        "output_summary = chain.run(docs)\n",
        "wrapped_text = textwrap.fill(output_summary, width=100)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "id": "VWqiTTC21w2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print( chain.llm_chain.prompt.template )"
      ],
      "metadata": {
        "id": "HX6nGFYB1zFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The \"stuff\" approach** is the simplest and most naive one, in which all the text from the transcribed video is used in a single prompt. This method may raise exceptions if all text is longer than the available context size of the LLM and may not be the most efficient way to handle large amounts of text.\n",
        "\n",
        "We’re going to experiment with the prompt below. This prompt will output the summary as bullet points."
      ],
      "metadata": {
        "id": "zvkhpo_J14lr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "n4RAWR6314pz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"Write a concise bullet point summary of the following:\n",
        "\n",
        "\n",
        "{text}\n",
        "\n",
        "\n",
        "CONSCISE SUMMARY IN BULLET POINTS:\"\"\"\n",
        "\n",
        "BULLET_POINT_PROMPT = PromptTemplate(template=prompt_template,\n",
        "                        input_variables=[\"text\"])"
      ],
      "metadata": {
        "id": "Ckjr9c4414Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, we initialized the summarization chain using the stuff as chain_type and the prompt above."
      ],
      "metadata": {
        "id": "zWXbHFPF19PP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(llm,\n",
        "                             chain_type=\"stuff\",\n",
        "                             prompt=BULLET_POINT_PROMPT)\n",
        "\n",
        "output_summary = chain.run(docs)\n",
        "\n",
        "wrapped_text = textwrap.fill(output_summary,\n",
        "                             width=1000,\n",
        "                             break_long_words=False,\n",
        "                             replace_whitespace=False)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "id": "g81qRNZT1_hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great job! By utilizing the provided prompt and implementing the appropriate summarization techniques, we've successfully obtained concise bullet-point summaries of the conversation.\n",
        "\n",
        "In LangChain we have the flexibility to create custom prompts tailored to specific needs. For instance, if you want the summarization output in French, you can easily construct a prompt that guides the language model to generate a summary in the desired language.\n",
        "\n",
        "**The 'refine'** summarization chain is a method for generating more accurate and context-aware summaries. This chain type is designed to iteratively refine the summary by providing additional context when needed. That means: it generates the summary of the first chunk. Then, for each successive chunk, the work-in-progress summary is integrated with new info from the new chunk.\n",
        "\n",
        "Copy\n"
      ],
      "metadata": {
        "id": "ZWzoqNGc2CjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
        "\n",
        "output_summary = chain.run(docs)\n",
        "wrapped_text = textwrap.fill(output_summary, width=100)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "id": "PBaDSH5a2OjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The 'refine' summarization chain in LangChain provides a flexible and **iterative approach to generating summaries, allowing you to customize prompts and provide additional context for refining the output. This method can result in more accurate and context-aware summaries compared to other **chain types like 'stuff' and 'map_reduce'**"
      ],
      "metadata": {
        "id": "AQl4dOAd2OwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Adding Transcripts to Deep Lake**\n",
        "This method can be extremely useful when you have more data. Let’s see how we can improve our expariment by adding multiple URLs, store them in Deep Lake database and retrieve information using QA chain.\n",
        "\n",
        "First, we need to modify the script for video downloading slightly, so it can work with a list of URLs."
      ],
      "metadata": {
        "id": "cgPQhVtc2ZZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yt_dlp\n",
        "\n",
        "def download_mp4_from_youtube(urls, job_id):\n",
        "    # This will hold the titles and authors of each downloaded video\n",
        "    video_info = []\n",
        "\n",
        "    for i, url in enumerate(urls):\n",
        "        # Set the options for the download\n",
        "        file_temp = f'./{job_id}_{i}.mp4'\n",
        "        ydl_opts = {\n",
        "            'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]',\n",
        "            'outtmpl': file_temp,\n",
        "            'quiet': True,\n",
        "        }\n",
        "\n",
        "        # Download the video file\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            result = ydl.extract_info(url, download=True)\n",
        "            title = result.get('title', \"\")\n",
        "            author = result.get('uploader', \"\")\n",
        "\n",
        "        # Add the title and author to our list\n",
        "        video_info.append((file_temp, title, author))\n",
        "\n",
        "    return video_info\n",
        "\n",
        "urls=[\"https://www.youtube.com/watch?v=mBjPyte2ZZo&t=78s\",\n",
        "    \"https://www.youtube.com/watch?v=cjs7QKJNVYM\",]\n",
        "vides_details = download_mp4_from_youtube(urls, 1)"
      ],
      "metadata": {
        "id": "V4mHJe952jgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "# load the model\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# iterate through each video and transcribe\n",
        "results = []\n",
        "for video in vides_details:\n",
        "    result = model.transcribe(video[0])\n",
        "    results.append( result['text'] )\n",
        "    print(f\"Transcription for {video[0]}:\\n{result['text']}\\n\")\n",
        "\n",
        "with open ('text.txt', 'w') as file:\n",
        "    file.write(results['text'])"
      ],
      "metadata": {
        "id": "Lea3S_j72lwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, load the texts from the file and use the text splitter to split the text to chunks with zero overlap before we store them in Deep Lake."
      ],
      "metadata": {
        "id": "hDMhraSu2sgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Load the texts\n",
        "with open('text.txt') as f:\n",
        "    text = f.read()\n",
        "texts = text_splitter.split_text(text)\n",
        "\n",
        "# Split the documents\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"]\n",
        "    )\n",
        "texts = text_splitter.split_text(text)"
      ],
      "metadata": {
        "id": "XSK3oOJ02qPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, as before we’ll pack all the chunks into a Documents:"
      ],
      "metadata": {
        "id": "sSMruxPQ2sEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document\n",
        "\n",
        "docs = [Document(page_content=t) for t in texts[:4]]"
      ],
      "metadata": {
        "id": "09XQDqUp2wjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import DeepLake\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
        "\n",
        "# create Deep Lake dataset\n",
        "# TODO: use your organization id here. (by default, org id is your username)\n",
        "my_activeloop_org_id = \"<YOUR-ACTIVELOOP-ORG-ID>\"\n",
        "my_activeloop_dataset_name = \"langchain_course_youtube_summarizer\"\n",
        "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
        "\n",
        "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
        "db.add_documents(docs)"
      ],
      "metadata": {
        "id": "0lHbEvD520Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In order to retrieve the information from the database, we’d have to construct a retriever object."
      ],
      "metadata": {
        "id": "8IB9b-rK259T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever()\n",
        "retriever.search_kwargs['distance_metric'] = 'cos'\n",
        "retriever.search_kwargs['k'] = 4"
      ],
      "metadata": {
        "id": "57IBSsMb29ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distance metric determines how the **Retriever measures \"distance\" or similarity between different data points** in the database. By setting distance_metric to **'cos', the Retriever will use cosine similarity** as its distance metric. **Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them.** It's often used in information retrieval to measure the similarity between documents or pieces of text. Also, by setting 'k' to 4, the Retriever will return the 4 most similar or closest results according to the distance metric when a search is performed.\n",
        "\n",
        "We can construct and use a custom prompt template with the QA chain. The RetrievalQA chain is useful to query similiar contents from databse and use the returned records as context to answer questions. The custom prompt ability gives us the flexibility to define custom tasks like retrieving the documents and summaizing the results in a bullet-point style.\n",
        "\n",
        "Copy\n"
      ],
      "metadata": {
        "id": "lkyBG3Nv3Egz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "prompt_template = \"\"\"Use the following pieces of transcripts from a video to answer the question in bullet points and summarized. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Summarized answer in bullter points:\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")"
      ],
      "metadata": {
        "id": "3GbR3DiC3Noj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we can use the chain_type_kwargs argument to define the custom prompt and for chain type the ‘stuff’  variation was picked. You can perform and test other types as well, as seen previously."
      ],
      "metadata": {
        "id": "_vUtrKor3QfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "qa = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                 chain_type=\"stuff\",\n",
        "                                 retriever=retriever,\n",
        "                                 chain_type_kwargs=chain_type_kwargs)\n",
        "\n",
        "print( qa.run(\"Summarize the mentions of google according to their AI program\") )"
      ],
      "metadata": {
        "id": "5G4ATdZp3D4O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}