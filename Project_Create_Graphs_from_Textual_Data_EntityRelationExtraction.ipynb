{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating Knowledge Graphs from Textual Data: Unveiling Hidden Connections**\n",
        "# **Introduction**\n",
        "In today's data-driven world, understanding the relationships between different pieces of information is crucial. Knowledge graphs have emerged as a powerful way to visualize and explore these connections, transforming unstructured text into a structured network of entities and their relationships. We will guide you through a simple workflow for creating a knowledge graph from textual data, making complex information more accessible and easier to understand"
      ],
      "metadata": {
        "id": "jGVBq3ezFVln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Workflow for Creating Knowledge Graphs from Textual Data**\n",
        "# Here’s what we are going to do in this project"
      ],
      "metadata": {
        "id": "fR1F_VuSFe4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Knowledge Graphs and Knowledge Bases: know the difference.**\n",
        "Before diving deep into our main topic, it's important to have a clear understanding of the difference between Knowledge Graphs and Knowledge Bases.\n",
        "\n",
        "The terms **\"knowledge graph\" and \"knowledge base\"** are often used interchangeably, but they have subtle differences.** Knowledge base (KB) refers to structured information** that we have about a domain of interest. On the other hand, a** knowledge graph is a knowledge base structured as a graph**, where nodes represent entities and edges signify relations between those entities. For example, from the text “Fabio lives in Italy,” we can extract the relation triplet <Fabio, lives in, Italy>, where “Fabio” and “Italy” are entities, and “lives in” it’s their relation.\n",
        "\n",
        "A knowledge graph is a particular type of knowledge base. A knowledge base is not necessarily a knowledge graph."
      ],
      "metadata": {
        "id": "7gPH0qZTFiKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building a Knowledge Graph**\n",
        "The process of building a knowledge graph usually consists of two sequential steps:\n",
        "\n",
        "**Named Entity Recognition (NER):** This step involves extracting entities from the text, which will eventually become the nodes of the knowledge graph.\n",
        "**Relation Classification (RC):** In this step, relations between entities are extracted, forming the edges of the knowledge graph.\n",
        "Then, the knowledge graph is commonly visualized using libraries such as pyvis"
      ],
      "metadata": {
        "id": "8-FXZlvyG6X9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Typically the process of creating a knowledge base from the text can be enhanced by incorporating additional steps, such as:\n",
        "\n",
        "**Entity Linking:** This involves normalizing entities to the same entity, such as “Napoleon” and “Napoleon Bonapart.” This is usually done by linking them to a canonical source, like a Wikipedia page.\n",
        "**Source Tracking:** Keeping track of the origin of each relation, such as the article URL and text span. Keeping track of the sources allows us to gather insights into the reliability of the extracted information (e.g., a relation is accurate if it can be extracted from several sources considered accurate).\n",
        "# In this project, we’ll do the Named Entity Recognition and Relation Classification tasks simultaneously with an appropriate prompt. This joint task is commonly called Relation Extraction (RE)."
      ],
      "metadata": {
        "id": "_ymCiyTrGat6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building a Knowledge Graph with LangChain**\n",
        "1. To demonstrate an example of using a prompt to extract relations from the text in LangChain, we can use the following  **KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT** prompt as a starting point. This prompt is designed to extract knowledge triples (subject, predicate, and object) from a given text input.\n",
        "2. This prompt can be used by the **ConversationEntityMemory class** from LangChain library, which is a way for chatbots to keep a memory of the past messages of a conversation by storing the relations extracted from the past messages.\n",
        " In this example, we use this prompt just to extract relations from texts without leveraging a memory class."
      ],
      "metadata": {
        "id": "ItPvOXE1HqXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain==0.1.4 deeplake openai==1.10.0 tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLASsfdvIKSa",
        "outputId": "dcbae47c-951f-404f-f66c-bca710a0485d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.1.4\n",
            "  Downloading langchain-0.1.4-py3-none-any.whl (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deeplake\n",
            "  Downloading deeplake-3.8.20.tar.gz (585 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.8/585.8 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting openai==1.10.0\n",
            "  Downloading openai-1.10.0-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.1.4)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.1.4)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.14 (from langchain==0.1.4)\n",
            "  Downloading langchain_community-0.0.20-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.16 (from langchain==0.1.4)\n",
            "  Downloading langchain_core-0.1.23-py3-none-any.whl (241 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1,>=0.0.83 (from langchain==0.1.4)\n",
            "  Downloading langsmith-0.0.92-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (8.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.10.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.10.0) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai==1.10.0)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.10.0) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.10.0) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai==1.10.0) (4.9.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from deeplake) (9.4.0)\n",
            "Collecting boto3 (from deeplake)\n",
            "  Downloading boto3-1.34.44-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from deeplake) (8.1.7)\n",
            "Collecting pathos (from deeplake)\n",
            "  Downloading pathos-0.3.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting humbug>=0.3.1 (from deeplake)\n",
            "  Downloading humbug-0.3.2-py3-none-any.whl (15 kB)\n",
            "Collecting lz4 (from deeplake)\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from deeplake) (2.3.0)\n",
            "Collecting libdeeplake==0.0.101 (from deeplake)\n",
            "  Downloading libdeeplake-0.0.101-cp310-cp310-manylinux2014_x86_64.whl (16.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aioboto3>=10.4.0 (from deeplake)\n",
            "  Downloading aioboto3-12.3.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from deeplake) (1.6.0)\n",
            "Collecting dill (from libdeeplake==0.0.101->deeplake)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Collecting aiobotocore[boto3]==2.11.2 (from aioboto3>=10.4.0->deeplake)\n",
            "  Downloading aiobotocore-2.11.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.34.35,>=1.33.2 (from aiobotocore[boto3]==2.11.2->aioboto3>=10.4.0->deeplake)\n",
            "  Downloading botocore-1.34.34-py3-none-any.whl (11.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.11.2->aioboto3>=10.4.0->deeplake) (1.14.1)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore[boto3]==2.11.2->aioboto3>=10.4.0->deeplake)\n",
            "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
            "Collecting boto3 (from deeplake)\n",
            "  Downloading boto3-1.34.34-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.10.0) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.10.0) (1.2.0)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->deeplake)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->deeplake)\n",
            "  Downloading s3transfer-0.10.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.4)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.4)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.10.0) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.10.0)\n",
            "  Downloading httpcore-1.0.3-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.10.0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.1.4)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting langsmith<0.1,>=0.0.83 (from langchain==0.1.4)\n",
            "  Downloading langsmith-0.0.87-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain==0.1.4) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.4) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.4) (2.16.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.4) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.4) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.4) (3.0.3)\n",
            "Collecting ppft>=1.7.6.8 (from pathos->deeplake)\n",
            "  Downloading ppft-1.7.6.8-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pox>=0.3.4 (from pathos->deeplake)\n",
            "  Downloading pox-0.3.4-py3-none-any.whl (29 kB)\n",
            "Collecting multiprocess>=0.70.16 (from pathos->deeplake)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.34.35,>=1.33.2->aiobotocore[boto3]==2.11.2->aioboto3>=10.4.0->deeplake) (2.8.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.4)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.35,>=1.33.2->aiobotocore[boto3]==2.11.2->aioboto3>=10.4.0->deeplake) (1.16.0)\n",
            "Building wheels for collected packages: deeplake\n",
            "  Building wheel for deeplake (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeplake: filename=deeplake-3.8.20-py3-none-any.whl size=704927 sha256=44922e35d2c2371d132e68713e505b4bf81a35866cbc98cccc7e815518d4bf6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/0c/45/76d618fadd335229ef5170a4cb3cb44fff23fdf6eca13173a0\n",
            "Successfully built deeplake\n",
            "Installing collected packages: ppft, pox, mypy-extensions, marshmallow, lz4, jsonpointer, jmespath, h11, dill, aioitertools, typing-inspect, tiktoken, multiprocess, libdeeplake, jsonpatch, humbug, httpcore, botocore, s3transfer, pathos, langsmith, httpx, dataclasses-json, aiobotocore, openai, langchain-core, boto3, langchain-community, langchain, aioboto3, deeplake\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aioboto3-12.3.0 aiobotocore-2.11.2 aioitertools-0.11.0 boto3-1.34.34 botocore-1.34.34 dataclasses-json-0.6.4 deeplake-3.8.20 dill-0.3.8 h11-0.14.0 httpcore-1.0.3 httpx-0.26.0 humbug-0.3.2 jmespath-1.0.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.4 langchain-community-0.0.20 langchain-core-0.1.23 langsmith-0.0.87 libdeeplake-0.0.101 lz4-4.3.3 marshmallow-3.20.2 multiprocess-0.70.16 mypy-extensions-1.0.0 openai-1.10.0 pathos-0.3.2 pox-0.3.4 ppft-1.7.6.8 s3transfer-0.10.0 tiktoken-0.6.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9yIZrl2HjF7",
        "outputId": "bc815bce-0e59-41a7-e68f-04ccd292e85a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv('/content/APIKeys.env')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-azIcBF0IgnM",
        "outputId": "c08671ce-0e2e-458e-872f-babe02bcb24f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_= load_dotenv(find_dotenv())\n",
        "\n",
        "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
        "GenerativeAIActiveLoop = os.environ['GenerativeAIActiveLoop']\n",
        "HUGGINGFACEHUB_API_TOKEN  = os.environ['HUGGINGFACEHUB_API_TOKEN']"
      ],
      "metadata": {
        "id": "hz8Q7zPCIhO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.graphs.networkx_graph import KG_TRIPLE_DELIMITER\n",
        "\n",
        "# Prompt template for knowledge triple extraction\n",
        "_DEFAULT_KNOWLEDGE_TRIPLE_EXTRACTION_TEMPLATE = (\n",
        "    \"You are a networked intelligence helping a human track knowledge triples\"\n",
        "    \" about all relevant people, things, concepts, etc. and integrating\"\n",
        "    \" them with your knowledge stored within your weights\"\n",
        "    \" as well as that stored in a knowledge graph.\"\n",
        "    \" Extract all of the knowledge triples from the text.\"\n",
        "    \" A knowledge triple is a clause that contains a subject, a predicate,\"\n",
        "    \" and an object. The subject is the entity being described,\"\n",
        "    \" the predicate is the property of the subject that is being\"\n",
        "    \" described, and the object is the value of the property.\\n\\n\"\n",
        "    \"EXAMPLE\\n\"\n",
        "    \"It's a state in the US. It's also the number 1 producer of gold in the US.\\n\\n\"\n",
        "    f\"Output: (Nevada, is a, state){KG_TRIPLE_DELIMITER}(Nevada, is in, US)\"\n",
        "    f\"{KG_TRIPLE_DELIMITER}(Nevada, is the number 1 producer of, gold)\\n\"\n",
        "    \"END OF EXAMPLE\\n\\n\"\n",
        "    \"EXAMPLE\\n\"\n",
        "    \"I'm going to the store.\\n\\n\"\n",
        "    \"Output: NONE\\n\"\n",
        "    \"END OF EXAMPLE\\n\\n\"\n",
        "    \"EXAMPLE\\n\"\n",
        "    \"Oh huh. I know Descartes likes to drive antique scooters and play the mandolin.\\n\"\n",
        "    f\"Output: (Descartes, likes to drive, antique scooters){KG_TRIPLE_DELIMITER}(Descartes, plays, mandolin)\\n\"\n",
        "    \"END OF EXAMPLE\\n\\n\"\n",
        "    \"EXAMPLE\\n\"\n",
        "    \"{text}\"\n",
        "    \"Output:\"\n",
        ")\n",
        "\n",
        "KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=_DEFAULT_KNOWLEDGE_TRIPLE_EXTRACTION_TEMPLATE,\n",
        ")\n",
        "\n",
        "# Make sure to save your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
        "# Instantiate the OpenAI model\n",
        "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.9)\n",
        "\n",
        "# Create an LLMChain using the knowledge triple extraction prompt\n",
        "chain = LLMChain(llm=llm, prompt=KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT)\n",
        "\n",
        "# Run the chain with the specified text\n",
        "text = \"The city of Paris is the capital and most populous city of France. The Eiffel Tower is a famous landmark in Paris.\"\n",
        "triples = chain.run(text)\n",
        "\n",
        "triples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CpPs9LEVIhRp",
        "outputId": "00427cff-8943-4d33-92a2-78a815f2e5e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' (Paris, is the capital of, France)<|>(Paris, is the most populous city of, France)<|>(Eiffel Tower, is a famous landmark in, Paris)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous code, we used the prompt to extract relation triplets from text using few-shot examples. We'll then parse the generated triplets and collect them into a list.\n",
        "\n",
        "Here, triples_list will contain the knowledge triplets extracted from the text. We need to parse the response and collect the triplets into a list:"
      ],
      "metadata": {
        "id": "6Wq5dw-jJwct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_triples(response, delimiter=KG_TRIPLE_DELIMITER):\n",
        "    if not response:\n",
        "        return []\n",
        "    return response.split(delimiter)\n",
        "\n",
        "triples_list = parse_triples(triples)\n",
        "\n",
        "# Print the extracted relation triplets\n",
        "print(triples_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWFeN4k8Jy4v",
        "outputId": "12a9d90e-8503-4a8b-ea7f-bbe6bf314f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' (Paris, is the capital of, France)', '(Paris, is the most populous city of, France)', '(Eiffel Tower, is a famous landmark in, Paris)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Knowledge Graph Visualization**\n",
        "**The NetworkX library** is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. It provides various g**raph generators, random graphs, and synthetic networks,** along with the benefits of Python's fast prototyping, ease of teaching, and multi-platform support.\n",
        "\n",
        "To visualize the extracted **triplets as a knowledge graph** we’ll be using the **pyvis library;** To install the library, execute the following command. While it is preferable to install the latest version of the packages, it is worth noting that the codes in this lesson were written using version 0.3.2."
      ],
      "metadata": {
        "id": "nK0wzL0EKAW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBhoeDleJt_G",
        "outputId": "10d561b9-731a-42c7-c21c-739f117b7b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvis\n",
            "  Downloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from pyvis) (7.34.0)\n",
            "Requirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.1.3)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.0.2)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.2.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=5.3.0->pyvis)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.9.6->pyvis) (2.1.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (0.2.13)\n",
            "Installing collected packages: jedi, pyvis\n",
            "Successfully installed jedi-0.19.1 pyvis-0.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvis.network import Network\n",
        "import networkx as nx"
      ],
      "metadata": {
        "id": "NiVneuX5KM5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a NetworkX graph from the extracted relation triplets\n",
        "def create_graph_from_triplets(triplets):\n",
        "    G = nx.DiGraph()\n",
        "    for triplet in triplets:\n",
        "        subject, predicate, obj = triplet.strip().split(',')\n",
        "        G.add_edge(subject.strip(), obj.strip(), label=predicate.strip())\n",
        "    return G"
      ],
      "metadata": {
        "id": "DCBhS_HBKUjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the NetworkX graph to a PyVis network\n",
        "def nx_to_pyvis(networkx_graph):\n",
        "    pyvis_graph = Network(notebook=True, cdn_resources='remote')\n",
        "    for node in networkx_graph.nodes():\n",
        "        pyvis_graph.add_node(node)\n",
        "    for edge in networkx_graph.edges(data=True):\n",
        "        pyvis_graph.add_edge(edge[0], edge[1], label=edge[2][\"label\"])\n",
        "    return pyvis_graph"
      ],
      "metadata": {
        "id": "tnBEUhhYKXOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "triplets = [t.strip() for t in triples_list if t.strip()]\n",
        "graph = create_graph_from_triplets(triplets)\n",
        "pyvis_network = nx_to_pyvis(graph)\n"
      ],
      "metadata": {
        "id": "IcxLTFAhKcMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Customize the appearance of the graph\n",
        "pyvis_network.toggle_hide_edges_on_drag(True)\n",
        "pyvis_network.toggle_physics(False)\n",
        "pyvis_network.set_edge_smooth('discrete')\n"
      ],
      "metadata": {
        "id": "XJIZypThKfFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the interactive knowledge graph visualization\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "pyvis_network.show('knowledge_graph.html')\n",
        "display(HTML('knowledge_graph.html'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "TlD11PnGMGc1",
        "outputId": "5c0b9823-f35a-4b76-c04b-30e1a674ebd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "knowledge_graph.html\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<html>\n",
              "    <head>\n",
              "        <meta charset=\"utf-8\">\n",
              "        \n",
              "            <script>function neighbourhoodHighlight(params) {\n",
              "  // console.log(\"in nieghbourhoodhighlight\");\n",
              "  allNodes = nodes.get({ returnType: \"Object\" });\n",
              "  // originalNodes = JSON.parse(JSON.stringify(allNodes));\n",
              "  // if something is selected:\n",
              "  if (params.nodes.length > 0) {\n",
              "    highlightActive = true;\n",
              "    var i, j;\n",
              "    var selectedNode = params.nodes[0];\n",
              "    var degrees = 2;\n",
              "\n",
              "    // mark all nodes as hard to read.\n",
              "    for (let nodeId in allNodes) {\n",
              "      // nodeColors[nodeId] = allNodes[nodeId].color;\n",
              "      allNodes[nodeId].color = \"rgba(200,200,200,0.5)\";\n",
              "      if (allNodes[nodeId].hiddenLabel === undefined) {\n",
              "        allNodes[nodeId].hiddenLabel = allNodes[nodeId].label;\n",
              "        allNodes[nodeId].label = undefined;\n",
              "      }\n",
              "    }\n",
              "    var connectedNodes = network.getConnectedNodes(selectedNode);\n",
              "    var allConnectedNodes = [];\n",
              "\n",
              "    // get the second degree nodes\n",
              "    for (i = 1; i < degrees; i++) {\n",
              "      for (j = 0; j < connectedNodes.length; j++) {\n",
              "        allConnectedNodes = allConnectedNodes.concat(\n",
              "          network.getConnectedNodes(connectedNodes[j])\n",
              "        );\n",
              "      }\n",
              "    }\n",
              "\n",
              "    // all second degree nodes get a different color and their label back\n",
              "    for (i = 0; i < allConnectedNodes.length; i++) {\n",
              "      // allNodes[allConnectedNodes[i]].color = \"pink\";\n",
              "      allNodes[allConnectedNodes[i]].color = \"rgba(150,150,150,0.75)\";\n",
              "      if (allNodes[allConnectedNodes[i]].hiddenLabel !== undefined) {\n",
              "        allNodes[allConnectedNodes[i]].label =\n",
              "          allNodes[allConnectedNodes[i]].hiddenLabel;\n",
              "        allNodes[allConnectedNodes[i]].hiddenLabel = undefined;\n",
              "      }\n",
              "    }\n",
              "\n",
              "    // all first degree nodes get their own color and their label back\n",
              "    for (i = 0; i < connectedNodes.length; i++) {\n",
              "      // allNodes[connectedNodes[i]].color = undefined;\n",
              "      allNodes[connectedNodes[i]].color = nodeColors[connectedNodes[i]];\n",
              "      if (allNodes[connectedNodes[i]].hiddenLabel !== undefined) {\n",
              "        allNodes[connectedNodes[i]].label =\n",
              "          allNodes[connectedNodes[i]].hiddenLabel;\n",
              "        allNodes[connectedNodes[i]].hiddenLabel = undefined;\n",
              "      }\n",
              "    }\n",
              "\n",
              "    // the main node gets its own color and its label back.\n",
              "    // allNodes[selectedNode].color = undefined;\n",
              "    allNodes[selectedNode].color = nodeColors[selectedNode];\n",
              "    if (allNodes[selectedNode].hiddenLabel !== undefined) {\n",
              "      allNodes[selectedNode].label = allNodes[selectedNode].hiddenLabel;\n",
              "      allNodes[selectedNode].hiddenLabel = undefined;\n",
              "    }\n",
              "  } else if (highlightActive === true) {\n",
              "    // console.log(\"highlightActive was true\");\n",
              "    // reset all nodes\n",
              "    for (let nodeId in allNodes) {\n",
              "      // allNodes[nodeId].color = \"purple\";\n",
              "      allNodes[nodeId].color = nodeColors[nodeId];\n",
              "      // delete allNodes[nodeId].color;\n",
              "      if (allNodes[nodeId].hiddenLabel !== undefined) {\n",
              "        allNodes[nodeId].label = allNodes[nodeId].hiddenLabel;\n",
              "        allNodes[nodeId].hiddenLabel = undefined;\n",
              "      }\n",
              "    }\n",
              "    highlightActive = false;\n",
              "  }\n",
              "\n",
              "  // transform the object into an array\n",
              "  var updateArray = [];\n",
              "  if (params.nodes.length > 0) {\n",
              "    for (let nodeId in allNodes) {\n",
              "      if (allNodes.hasOwnProperty(nodeId)) {\n",
              "        // console.log(allNodes[nodeId]);\n",
              "        updateArray.push(allNodes[nodeId]);\n",
              "      }\n",
              "    }\n",
              "    nodes.update(updateArray);\n",
              "  } else {\n",
              "    // console.log(\"Nothing was selected\");\n",
              "    for (let nodeId in allNodes) {\n",
              "      if (allNodes.hasOwnProperty(nodeId)) {\n",
              "        // console.log(allNodes[nodeId]);\n",
              "        // allNodes[nodeId].color = {};\n",
              "        updateArray.push(allNodes[nodeId]);\n",
              "      }\n",
              "    }\n",
              "    nodes.update(updateArray);\n",
              "  }\n",
              "}\n",
              "\n",
              "function filterHighlight(params) {\n",
              "  allNodes = nodes.get({ returnType: \"Object\" });\n",
              "  // if something is selected:\n",
              "  if (params.nodes.length > 0) {\n",
              "    filterActive = true;\n",
              "    let selectedNodes = params.nodes;\n",
              "\n",
              "    // hiding all nodes and saving the label\n",
              "    for (let nodeId in allNodes) {\n",
              "      allNodes[nodeId].hidden = true;\n",
              "      if (allNodes[nodeId].savedLabel === undefined) {\n",
              "        allNodes[nodeId].savedLabel = allNodes[nodeId].label;\n",
              "        allNodes[nodeId].label = undefined;\n",
              "      }\n",
              "    }\n",
              "\n",
              "    for (let i=0; i < selectedNodes.length; i++) {\n",
              "      allNodes[selectedNodes[i]].hidden = false;\n",
              "      if (allNodes[selectedNodes[i]].savedLabel !== undefined) {\n",
              "        allNodes[selectedNodes[i]].label = allNodes[selectedNodes[i]].savedLabel;\n",
              "        allNodes[selectedNodes[i]].savedLabel = undefined;\n",
              "      }\n",
              "    }\n",
              "\n",
              "  } else if (filterActive === true) {\n",
              "    // reset all nodes\n",
              "    for (let nodeId in allNodes) {\n",
              "      allNodes[nodeId].hidden = false;\n",
              "      if (allNodes[nodeId].savedLabel !== undefined) {\n",
              "        allNodes[nodeId].label = allNodes[nodeId].savedLabel;\n",
              "        allNodes[nodeId].savedLabel = undefined;\n",
              "      }\n",
              "    }\n",
              "    filterActive = false;\n",
              "  }\n",
              "\n",
              "  // transform the object into an array\n",
              "  var updateArray = [];\n",
              "  if (params.nodes.length > 0) {\n",
              "    for (let nodeId in allNodes) {\n",
              "      if (allNodes.hasOwnProperty(nodeId)) {\n",
              "        updateArray.push(allNodes[nodeId]);\n",
              "      }\n",
              "    }\n",
              "    nodes.update(updateArray);\n",
              "  } else {\n",
              "    for (let nodeId in allNodes) {\n",
              "      if (allNodes.hasOwnProperty(nodeId)) {\n",
              "        updateArray.push(allNodes[nodeId]);\n",
              "      }\n",
              "    }\n",
              "    nodes.update(updateArray);\n",
              "  }\n",
              "}\n",
              "\n",
              "function selectNode(nodes) {\n",
              "  network.selectNodes(nodes);\n",
              "  neighbourhoodHighlight({ nodes: nodes });\n",
              "  return nodes;\n",
              "}\n",
              "\n",
              "function selectNodes(nodes) {\n",
              "  network.selectNodes(nodes);\n",
              "  filterHighlight({nodes: nodes});\n",
              "  return nodes;\n",
              "}\n",
              "\n",
              "function highlightFilter(filter) {\n",
              "  let selectedNodes = []\n",
              "  let selectedProp = filter['property']\n",
              "  if (filter['item'] === 'node') {\n",
              "    let allNodes = nodes.get({ returnType: \"Object\" });\n",
              "    for (let nodeId in allNodes) {\n",
              "      if (allNodes[nodeId][selectedProp] && filter['value'].includes((allNodes[nodeId][selectedProp]).toString())) {\n",
              "        selectedNodes.push(nodeId)\n",
              "      }\n",
              "    }\n",
              "  }\n",
              "  else if (filter['item'] === 'edge'){\n",
              "    let allEdges = edges.get({returnType: 'object'});\n",
              "    // check if the selected property exists for selected edge and select the nodes connected to the edge\n",
              "    for (let edge in allEdges) {\n",
              "      if (allEdges[edge][selectedProp] && filter['value'].includes((allEdges[edge][selectedProp]).toString())) {\n",
              "        selectedNodes.push(allEdges[edge]['from'])\n",
              "        selectedNodes.push(allEdges[edge]['to'])\n",
              "      }\n",
              "    }\n",
              "  }\n",
              "  selectNodes(selectedNodes)\n",
              "}</script>\n",
              "            <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css\" integrity=\"sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\" />\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js\" integrity=\"sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\"></script>\n",
              "            \n",
              "            \n",
              "            \n",
              "            \n",
              "            \n",
              "            \n",
              "\n",
              "        \n",
              "<center>\n",
              "<h1></h1>\n",
              "</center>\n",
              "\n",
              "<!-- <link rel=\"stylesheet\" href=\"../node_modules/vis/dist/vis.min.css\" type=\"text/css\" />\n",
              "<script type=\"text/javascript\" src=\"../node_modules/vis/dist/vis.js\"> </script>-->\n",
              "        <link\n",
              "          href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css\"\n",
              "          rel=\"stylesheet\"\n",
              "          integrity=\"sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6\"\n",
              "          crossorigin=\"anonymous\"\n",
              "        />\n",
              "        <script\n",
              "          src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js\"\n",
              "          integrity=\"sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf\"\n",
              "          crossorigin=\"anonymous\"\n",
              "        ></script>\n",
              "\n",
              "\n",
              "        <center>\n",
              "          <h1></h1>\n",
              "        </center>\n",
              "        <style type=\"text/css\">\n",
              "\n",
              "             #mynetwork {\n",
              "                 width: 100%;\n",
              "                 height: 600px;\n",
              "                 background-color: #ffffff;\n",
              "                 border: 1px solid lightgray;\n",
              "                 position: relative;\n",
              "                 float: left;\n",
              "             }\n",
              "\n",
              "             \n",
              "\n",
              "             \n",
              "\n",
              "             \n",
              "        </style>\n",
              "    </head>\n",
              "\n",
              "\n",
              "    <body>\n",
              "        <div class=\"card\" style=\"width: 100%\">\n",
              "            \n",
              "            \n",
              "            <div id=\"mynetwork\" class=\"card-body\"></div>\n",
              "        </div>\n",
              "\n",
              "        \n",
              "        \n",
              "\n",
              "        <script type=\"text/javascript\">\n",
              "\n",
              "              // initialize global variables.\n",
              "              var edges;\n",
              "              var nodes;\n",
              "              var allNodes;\n",
              "              var allEdges;\n",
              "              var nodeColors;\n",
              "              var originalNodes;\n",
              "              var network;\n",
              "              var container;\n",
              "              var options, data;\n",
              "              var filter = {\n",
              "                  item : '',\n",
              "                  property : '',\n",
              "                  value : []\n",
              "              };\n",
              "\n",
              "              \n",
              "\n",
              "              \n",
              "\n",
              "              // This method is responsible for drawing the graph, returns the drawn network\n",
              "              function drawGraph() {\n",
              "                  var container = document.getElementById('mynetwork');\n",
              "\n",
              "                  \n",
              "\n",
              "                  // parsing and collecting nodes and edges from the python\n",
              "                  nodes = new vis.DataSet([{\"color\": \"#97c2fc\", \"id\": \"(Paris\", \"label\": \"(Paris\", \"shape\": \"dot\"}, {\"color\": \"#97c2fc\", \"id\": \"France)\", \"label\": \"France)\", \"shape\": \"dot\"}, {\"color\": \"#97c2fc\", \"id\": \"(Eiffel Tower\", \"label\": \"(Eiffel Tower\", \"shape\": \"dot\"}, {\"color\": \"#97c2fc\", \"id\": \"Paris)\", \"label\": \"Paris)\", \"shape\": \"dot\"}]);\n",
              "                  edges = new vis.DataSet([{\"from\": \"(Paris\", \"label\": \"is the most populous city of\", \"to\": \"France)\"}, {\"from\": \"(Eiffel Tower\", \"label\": \"is a famous landmark in\", \"to\": \"Paris)\"}]);\n",
              "\n",
              "                  nodeColors = {};\n",
              "                  allNodes = nodes.get({ returnType: \"Object\" });\n",
              "                  for (nodeId in allNodes) {\n",
              "                    nodeColors[nodeId] = allNodes[nodeId].color;\n",
              "                  }\n",
              "                  allEdges = edges.get({ returnType: \"Object\" });\n",
              "                  // adding nodes and edges to the graph\n",
              "                  data = {nodes: nodes, edges: edges};\n",
              "\n",
              "                  var options = {\n",
              "    \"configure\": {\n",
              "        \"enabled\": false\n",
              "    },\n",
              "    \"edges\": {\n",
              "        \"color\": {\n",
              "            \"inherit\": true\n",
              "        },\n",
              "        \"smooth\": {\n",
              "            \"enabled\": true,\n",
              "            \"type\": \"discrete\"\n",
              "        }\n",
              "    },\n",
              "    \"interaction\": {\n",
              "        \"dragNodes\": true,\n",
              "        \"hideEdgesOnDrag\": true,\n",
              "        \"hideNodesOnDrag\": false\n",
              "    },\n",
              "    \"physics\": {\n",
              "        \"enabled\": false,\n",
              "        \"stabilization\": {\n",
              "            \"enabled\": true,\n",
              "            \"fit\": true,\n",
              "            \"iterations\": 1000,\n",
              "            \"onlyDynamicEdges\": false,\n",
              "            \"updateInterval\": 50\n",
              "        }\n",
              "    }\n",
              "};\n",
              "\n",
              "                  \n",
              "\n",
              "\n",
              "                  \n",
              "\n",
              "                  network = new vis.Network(container, data, options);\n",
              "\n",
              "                  \n",
              "\n",
              "                  \n",
              "\n",
              "                  \n",
              "\n",
              "\n",
              "                  \n",
              "\n",
              "                  return network;\n",
              "\n",
              "              }\n",
              "              drawGraph();\n",
              "        </script>\n",
              "    </body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we defined two functions for creating and visualizing a knowledge graph from a list of relation triplets; then, we used the triples_list to create a list of cleaned triplets which creates a NetworkX graph and converts it to a PyVis network. It also customizes the graph's appearance by enabling edge hiding on drag, disabling physics, and setting edge smoothing to 'discrete.’\n",
        "\n",
        "With that process, we generated an interactive HTML file named knowledge_graph.html containing the knowledge graph visualization based on the extracted relation triplets:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qDp0ePbvLZpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "Throughout this article, we've demonstrated a straightforward yet powerful workflow for creating knowledge graphs from textual data. We've transformed unstructured text into a structured network of entities and their relationships, making complex information more accessible and easier to understand.\n",
        "\n",
        "It's worth noting that LangChain offers the GraphIndexCreator class, which automates the extraction of relation triplets and is seamlessly integrated with question-answering chains. In future articles, we'll delve deeper into this powerful feature, showcasing its potential further to enhance your knowledge graph creation and analysis capabilities.\n",
        "\n",
        "The knowledge graph created through this workflow serves as a valuable tool for visualizing complex relationships and opens the door for further analysis, pattern recognition, and data-driven decision-making.\n",
        "\n",
        "Congrats in completing the second module of the course! You can now test your knowledge with the module quizzes. The next module will be about managing external knowledge in LLM-based applications using indexers and retrievers.\n",
        "\n"
      ],
      "metadata": {
        "id": "CEKNIiXeOrKp"
      }
    }
  ]
}