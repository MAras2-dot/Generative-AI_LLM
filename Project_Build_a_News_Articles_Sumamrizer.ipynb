{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Build a News Articles Summarizer**\n",
        "\n",
        "Introduction\n",
        "In today's fast-paced world, it's essential to stay updated with the latest news and information. However, going through multiple news articles can be time-consuming. To help you save time and get a quick overview of the important points, let’s develop a News Articles Summarizer application using ChatGPT and LangChain. With this powerful tool, we can scrape online articles, extract their titles and text, and generate concise summaries. Within this lesson, we will walk you through the workflow of constructing a summarizer. We will employ the concepts we discussed in earlier lessons, demonstrating their application in a real-world scenario.\n",
        "\n",
        "Workflow for Building a News Articles Summarizer\n",
        "Here’s what we are going to do in this project.\n",
        "\n"
      ],
      "metadata": {
        "id": "d1xZqwc1sxzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here are the steps described in more detail:\n",
        "\n",
        "\n",
        "1.   Install required libraries: To get started, ensure you have the necessary libraries installed: **requests, newspaper3k, and langchain**.\n",
        "2.  Scrape articles: Use **the requests library to scrape** the content of the target news articles from their respective URLs.\n",
        "3. Extract titles and text: Employ the **newspaper library to parse the scraped HTML** and extract the titles and text of the articles.\n",
        "4.Preprocess the text: **Clean and preprocess the extracted texts** to make them suitable for input to ChatGPT.\n",
        "5. Generate summaries: **Utilize ChatGPT to summarize** the extracted articles' text concisely.\n",
        "6. Output the results: **Present the summaries** along with the original titles, allowing users to grasp the main points of each article quickly.\n"
      ],
      "metadata": {
        "id": "O1hulYe4s8T3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain==0.1.4 deeplake openai==1.10.0 tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM_ZzAUKt4ck",
        "outputId": "8aefa869-0067-428b-9c19-a5b81bd4a5d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain==0.1.4 in /usr/local/lib/python3.10/dist-packages (0.1.4)\n",
            "Requirement already satisfied: deeplake in /usr/local/lib/python3.10/dist-packages (3.8.20)\n",
            "Requirement already satisfied: openai==1.10.0 in /usr/local/lib/python3.10/dist-packages (1.10.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.14 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (0.0.20)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (0.1.23)\n",
            "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (0.0.87)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (8.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.10.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.10.0) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.10.0) (0.26.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.10.0) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.10.0) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai==1.10.0) (4.9.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from deeplake) (9.4.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from deeplake) (1.34.34)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from deeplake) (8.1.7)\n",
            "Requirement already satisfied: pathos in /usr/local/lib/python3.10/dist-packages (from deeplake) (0.3.2)\n",
            "Requirement already satisfied: humbug>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from deeplake) (0.3.2)\n",
            "Requirement already satisfied: lz4 in /usr/local/lib/python3.10/dist-packages (from deeplake) (4.3.3)\n",
            "Requirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from deeplake) (2.3.0)\n",
            "Requirement already satisfied: libdeeplake==0.0.101 in /usr/local/lib/python3.10/dist-packages (from deeplake) (0.0.101)\n",
            "Requirement already satisfied: aioboto3>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from deeplake) (12.3.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from deeplake) (1.6.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from libdeeplake==0.0.101->deeplake) (0.3.8)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: aiobotocore[boto3]==2.11.2 in /usr/local/lib/python3.10/dist-packages (from aioboto3>=10.4.0->deeplake) (2.11.2)\n",
            "Requirement already satisfied: botocore<1.34.35,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.11.2->aioboto3>=10.4.0->deeplake) (1.34.34)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.11.2->aioboto3>=10.4.0->deeplake) (1.14.1)\n",
            "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.11.2->aioboto3>=10.4.0->deeplake) (0.11.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.10.0) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.10.0) (1.2.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->deeplake) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3->deeplake) (0.10.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.4) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.4) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.10.0) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.10.0) (1.0.3)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.10.0) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.4) (2.4)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain==0.1.4) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.4) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.4) (2.16.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.4) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.4) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.4) (3.0.3)\n",
            "Requirement already satisfied: ppft>=1.7.6.8 in /usr/local/lib/python3.10/dist-packages (from pathos->deeplake) (1.7.6.8)\n",
            "Requirement already satisfied: pox>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from pathos->deeplake) (0.3.4)\n",
            "Requirement already satisfied: multiprocess>=0.70.16 in /usr/local/lib/python3.10/dist-packages (from pathos->deeplake) (0.70.16)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.34.35,>=1.33.2->aiobotocore[boto3]==2.11.2->aioboto3>=10.4.0->deeplake) (2.8.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.4) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.35,>=1.33.2->aiobotocore[boto3]==2.11.2->aioboto3>=10.4.0->deeplake) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FuUVczbuTcS",
        "outputId": "2930114a-d31f-4202-aacd-a288a0259dc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv('/content/APIKeys.env')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHotir5UuWRT",
        "outputId": "dd54dca0-c952-47ac-bcaa-9a6d1c48b084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_= load_dotenv(find_dotenv())\n",
        "\n",
        "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
        "GenerativeAIActiveLoop = os.environ['GenerativeAIActiveLoop']"
      ],
      "metadata": {
        "id": "Pzt5k0hTuWkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We picked the URL of a news article to generate a summary. The following code fetches articles from a list of URLs using the requests library with a custom User-Agent header. It then extracts the title and text of each article using the newspaper library."
      ],
      "metadata": {
        "id": "SoS8K3krumwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q newspaper3k"
      ],
      "metadata": {
        "id": "GdI7I73mu2d1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from newspaper import Article"
      ],
      "metadata": {
        "id": "NucWzAZAuWnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from newspaper import Article\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n",
        "}\n",
        "\n",
        "article_url = \"https://www.artificialintelligence-news.com/2022/01/25/meta-claims-new-ai-supercomputer-will-set-records/\"\n",
        "\n",
        "session = requests.Session()\n",
        "\n",
        "try:\n",
        "    response = session.get(article_url, headers=headers, timeout=10)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        article = Article(article_url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "\n",
        "        print(f\"Title: {article.title}\")\n",
        "        print(f\"Text: {article.text}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to fetch article at {article_url}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error occurred while fetching article at {article_url}: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msYTkgeSuWpr",
        "outputId": "50c3756b-a696-4c12-fabd-f7f79126c2b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Meta claims its new AI supercomputer will set records\n",
            "Text: Ryan is a senior editor at TechForge Media with over a decade of experience covering the latest technology and interviewing leading industry figures. He can often be sighted at tech conferences with a strong coffee in one hand and a laptop in the other. If it's geeky, he’s probably into it. Find him on Twitter (@Gadget_Ry) or Mastodon (@gadgetry@techhub.social)\n",
            "\n",
            "Meta (formerly Facebook) has unveiled an AI supercomputer that it claims will be the world’s fastest.\n",
            "\n",
            "The supercomputer is called the AI Research SuperCluster (RSC) and is yet to be fully complete. However, Meta’s researchers have already begun using it for training large natural language processing (NLP) and computer vision models.\n",
            "\n",
            "RSC is set to be fully built in mid-2022. Meta says that it will be the fastest in the world once complete and the aim is for it to be capable of training models with trillions of parameters.\n",
            "\n",
            "“We hope RSC will help us build entirely new AI systems that can, for example, power real-time voice translations to large groups of people, each speaking a different language, so they can seamlessly collaborate on a research project or play an AR game together,” wrote Meta in a blog post.\n",
            "\n",
            "“Ultimately, the work done with RSC will pave the way toward building technologies for the next major computing platform — the metaverse, where AI-driven applications and products will play an important role.”\n",
            "\n",
            "For production, Meta expects RSC will be 20x faster than Meta’s current V100-based clusters. RSC is also estimated to be 9x faster at running the NVIDIA Collective Communication Library (NCCL) and 3x faster at training large-scale NLP workflows.\n",
            "\n",
            "A model with tens of billions of parameters can finish training in three weeks compared with nine weeks prior to RSC.\n",
            "\n",
            "Meta says that its previous AI research infrastructure only leveraged open source and other publicly-available datasets. RSC was designed with the security and privacy controls in mind to allow Meta to use real-world examples from its production systems in production training.\n",
            "\n",
            "What this means in practice is that Meta can use RSC to advance research for vital tasks such as identifying harmful content on its platforms—using real data from them.\n",
            "\n",
            "“We believe this is the first time performance, reliability, security, and privacy have been tackled at such a scale,” says Meta.\n",
            "\n",
            "(Image Credit: Meta)\n",
            "\n",
            "Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo. The next events in the series will be held in Santa Clara on 11-12 May 2022, Amsterdam on 20-21 September 2022, and London on 1-2 December 2022.\n",
            "\n",
            "Explore other upcoming enterprise technology events and webinars powered by TechForge here.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next code imports essential classes and functions from the LangChain and sets up a ChatOpenAI instance with a temperature of 0 for controlled response generation. Additionally, it imports chat-related message schema classes, which enable the smooth handling of chat-based tasks. The following code will start by setting the prompt and filling it with the article’s conten"
      ],
      "metadata": {
        "id": "z7jT5XnKvca7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import(\n",
        "    HumanMessage\n",
        "    )\n",
        "\n",
        "# we get the article data from the scraping part\n",
        "article_title = article.title\n",
        "article_text = article.text\n",
        "\n",
        "\n",
        "# prepare template for prompt\n",
        "template = \"\"\"You are a very good assistant that summarizes online articles.\n",
        "\n",
        "Here's the article you want to summarize.\n",
        "\n",
        "==================\n",
        "Title: {article_title}\n",
        "\n",
        "{article_text}\n",
        "==================\n",
        "\n",
        "Write a summary of the previous article.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = template.format(article_title=article.title, article_text=article.text)\n",
        "\n",
        "messages = [HumanMessage(content=prompt)]"
      ],
      "metadata": {
        "id": "CZGcLdpJvdAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **HumanMessage** is a structured data format representing user messages within the chat-based interaction framework. The **ChatOpenAI class** is utilized to interact with the AI model, while the **HumanMessage schema **provides a standardized representation of user messages. **The template consists of placeholders **for the **article's title and content**, which will be substituted with the actual article_title and article_text. This process simplifies and streamlines the creation of **dynamic prompts** by allowing you to define a template with placeholders and then replace them with actual data when needed."
      ],
      "metadata": {
        "id": "sukADsCIwZMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "#load the model\n",
        "chat= ChatOpenAI(model_name = \"gpt-3.5-turbo\" , temperature =0)"
      ],
      "metadata": {
        "id": "ZgrYnIMuwue-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we loaded the model and set the temperature to 0. We’d use the chat() instance to generate a summary by passing a single HumanMessage object containing the formatted prompt. The AI model processes this prompt and returns a concise summary:"
      ],
      "metadata": {
        "id": "fUSP6Ub6xK74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#generate summary\n",
        "summary =chat(messages)\n",
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4n34wzZxL-s",
        "outputId": "5697e88f-6ad6-47bd-a4ca-c3461846e404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Meta (formerly Facebook) has unveiled a new AI supercomputer called the AI Research SuperCluster (RSC) that is set to be the world's fastest once fully built in mid-2022. The supercomputer will be capable of training models with trillions of parameters and is expected to be 20x faster than Meta's current clusters. Meta aims to use RSC for tasks such as real-time voice translations and identifying harmful content on its platforms. The supercomputer was designed with security and privacy controls in mind to allow Meta to use real-world examples from its production systems in training.\")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# If we want a bulleted list, we can modify a prompt and get the result."
      ],
      "metadata": {
        "id": "XCw99eIEzAhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare template for prompt\n",
        "template = \"\"\"You are an advanced AI assistant that summarizes online articles into bulleted lists.\n",
        "\n",
        "Here's the article you need to summarize.\n",
        "\n",
        "==================\n",
        "Title: {article_title}\n",
        "\n",
        "{article_text}\n",
        "==================\n",
        "\n",
        "Now, provide a summarized version of the article in a bulleted list format.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# format prompt\n",
        "prompt = template.format(article_title=article.title, article_text=article.text)\n",
        "\n",
        "# generate summary\n",
        "summary = chat([HumanMessage(content=prompt)])\n",
        "print(summary.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEKqZuTjy_1w",
        "outputId": "d087f8f6-a119-456b-b4ec-1623ccfbef27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Meta (formerly Facebook) has unveiled an AI supercomputer called the AI Research SuperCluster (RSC) that it claims will be the world's fastest.\n",
            "- RSC is still under construction but Meta's researchers have already started using it for training large NLP and computer vision models.\n",
            "- Once fully built in mid-2022, RSC is expected to be capable of training models with trillions of parameters and be 20x faster than Meta's current clusters.\n",
            "- Meta aims to use RSC to develop AI systems for applications like real-time voice translations and AR games in the metaverse.\n",
            "- RSC is designed with security and privacy controls to allow Meta to use real-world data from its production systems for training.\n",
            "- Meta believes that RSC's performance, reliability, security, and privacy features are unprecedented at such a scale.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qUJAmJt71UKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from newspaper import Article\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n",
        "}\n",
        "\n",
        "article_url = \"https://www.cnbc.com/2024/02/16/tech-and-ai-companies-sign-accord-to-combat-election-related-deepfakes.html\"\n",
        "\n",
        "session = requests.Session()\n",
        "\n",
        "try:\n",
        "    response = session.get(article_url, headers=headers, timeout=10)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        article = Article(article_url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "\n",
        "        print(f\"Title: {article.title}\")\n",
        "        print(f\"Text: {article.text}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to fetch article at {article_url}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error occurred while fetching article at {article_url}: {e}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ptr3ejGO1IpU",
        "outputId": "a64ae6ce-3047-489e-a4ed-e5c495c54d95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Microsoft, Google, Amazon and tech peers sign pact to combat election-related misinformation\n",
            "Text: Sam Altman, CEO of OpenAI, attends the 54th annual meeting of the World Economic Forum, in Davos, Switzerland, on Jan. 18, 2024.\n",
            "\n",
            "A group of 20 leading tech companies on Friday announced a joint commitment to combat AI misinformation in this year's elections.\n",
            "\n",
            "The industry is specifically targeting deepfakes, which can use deceptive audio, video and images to mimic key stakeholders in democratic elections or to provide false voting information.\n",
            "\n",
            "Microsoft , Meta , Google , Amazon , IBM , Adobe and chip designer Arm all signed the accord. Artificial intelligence startups OpenAI, Anthropic and Stability AI also joined the group, alongside social media companies such as Snap , TikTok and X.\n",
            "\n",
            "Tech platforms are preparing for a huge year of elections around the world that affect upward of four billion people in more than 40 countries. The rise of AI-generated content has led to serious election-related misinformation concerns, with the number of deepfakes that have been created increasing 900% year over year, according to data from Clarity, a machine learning firm.\n",
            "\n",
            "Misinformation in elections has been a major problem dating back to the 2016 presidential campaign, when Russian actors found cheap and easy ways to spread inaccurate content across social platforms. Lawmakers are even more concerned today with the rapid rise of AI.\n",
            "\n",
            "\"There is reason for serious concern about how AI could be used to mislead voters in campaigns,\" said Josh Becker, a Democratic state senator in California, in an interview. \"It's encouraging to see some companies coming to the table but right now I don't see enough specifics, so we will likely need legislation that sets clear standards.\"\n",
            "\n",
            "Meanwhile, the detection and watermarking technologies used for identifying deepfakes haven't advanced quickly enough to keep up. For now, the companies are just agreeing on what amounts to a set of technical standards and detection mechanisms.\n",
            "\n",
            "They have a long way to go to effectively combat the problem, which has many layers. Services that claim to identify AI-generated text, such as essays, for instance, have been shown to exhibit bias against non-native English speakers. And it's not much easier for images and videos.\n",
            "\n",
            "Even if platforms behind AI-generated images and videos agree to bake in things like invisible watermarks and certain types of metadata, there are ways around those protective measures. Screenshotting can even sometimes dupe a detector.\n",
            "\n",
            "Additionally, the invisible signals that some companies include in AI-generated images haven't yet made it to many audio and video generators.\n",
            "\n",
            "News of the accord comes a day after ChatGPT creator OpenAI announced Sora, its new model for AI-generated video. Sora works similarly to OpenAI's image-generation AI tool, DALL-E. A user types out a desired scene and Sora will return a high-definition video clip. Sora can also generate video clips inspired by still images, and extend existing videos or fill in missing frames.\n",
            "\n",
            "Participating companies in the accord agreed to eight high-level commitments, including assessing model risks, \"seeking to detect\" and address the distribution of such content on their platforms and providing transparency on those processes to the public. As with most voluntary commitments in the tech industry and beyond, the release specified that the commitments apply only \"where they are relevant for services each company provides.\"\n",
            "\n",
            "\"Democracy rests on safe and secure elections,\" Kent Walker, Google's president of global affairs, said in a release. The accord reflects the industry's effort to take on \"AI-generated election misinformation that erodes trust,\" he said.\n",
            "\n",
            "Christina Montgomery, IBM's chief privacy and trust officer, said in the release that in this key election year, \"concrete, cooperative measures are needed to protect people and societies from the amplified risks of AI-generated deceptive content.\"\n",
            "\n",
            "WATCH: OpenAI unveils Sora\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare template for prompt\n",
        "template = \"\"\"You are an advanced AI assistant that summarizes online articles into bulleted lists.\n",
        "\n",
        "Here's the article you need to summarize.\n",
        "\n",
        "==================\n",
        "Title: {article_title}\n",
        "\n",
        "{article_text}\n",
        "==================\n",
        "\n",
        "Now, provide a summarized version of the article in a bulleted list format.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# format prompt\n",
        "prompt = template.format(article_title=article.title, article_text=article.text)\n",
        "\n",
        "# generate summary\n",
        "summary = chat([HumanMessage(content=prompt)])\n",
        "print(summary.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Qd4dmr21VBE",
        "outputId": "00e5cdfd-f0de-4622-da8b-3c4cb3b8b777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Meta (formerly Facebook) has unveiled an AI supercomputer called the AI Research SuperCluster (RSC) that it claims will be the world's fastest.\n",
            "- RSC is still under construction but is already being used for training large NLP and computer vision models.\n",
            "- Meta aims for RSC to be capable of training models with trillions of parameters and to power AI systems for real-time voice translations and AR applications in the metaverse.\n",
            "- Meta expects RSC to be 20x faster than its current clusters, 9x faster at running the NVIDIA Collective Communication Library, and 3x faster at training large-scale NLP workflows.\n",
            "- RSC will enable models with tens of billions of parameters to finish training in three weeks compared to nine weeks prior to RSC.\n",
            "- RSC was designed with security and privacy controls to allow Meta to use real-world data from its production systems for training AI models.\n",
            "- Meta believes this is the first time performance, reliability, security, and privacy have been tackled at such a scale in AI research infrastructure.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Instruct the model - Ask Prompt to translate in French"
      ],
      "metadata": {
        "id": "dseu-Nsq13VI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare template for prompt\n",
        "template = \"\"\"You are an advanced AI assistant that summarizes online articles into bulleted lists in French.\n",
        "\n",
        "Here's the article you need to summarize.\n",
        "\n",
        "==================\n",
        "Title: {article_title}\n",
        "\n",
        "{article_text}\n",
        "==================\n",
        "\n",
        "Now, provide a summarized version of the article in a bulleted list format.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# format prompt\n",
        "prompt = template.format(article_title=article.title, article_text=article.text)\n",
        "\n",
        "# generate summary\n",
        "summary = chat([HumanMessage(content=prompt)])\n",
        "print(summary.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fjl9etnE1x4x",
        "outputId": "268cf93f-8f02-464f-ce65-0fa2addc67a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Meta (anciennement Facebook) a dévoilé un superordinateur d'IA appelé le AI Research SuperCluster (RSC) qui sera le plus rapide au monde une fois terminé en 2022.\n",
            "- Le RSC est utilisé pour former de grands modèles de traitement du langage naturel (NLP) et de vision par ordinateur.\n",
            "- Meta espère que le RSC permettra de construire de nouveaux systèmes d'IA pour des applications telles que les traductions vocales en temps réel et les jeux en réalité augmentée.\n",
            "- Le RSC devrait être 20 fois plus rapide que les clusters actuels de Meta, 9 fois plus rapide pour exécuter la NVIDIA Collective Communication Library (NCCL) et 3 fois plus rapide pour former des flux de travail NLP à grande échelle.\n",
            "- Meta utilise le RSC pour avancer dans la recherche sur des tâches vitales telles que l'identification de contenus nuisibles sur ses plateformes en utilisant de vraies données.\n",
            "- Le RSC a été conçu avec des contrôles de sécurité et de confidentialité pour permettre à Meta d'utiliser des exemples du monde réel de ses systèmes de production dans la formation en production.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Output Parsers**\n",
        "Now, let’s improve the previous section by using Output Parsers. The Pydantic output parser in LangChain offers a flexible way to shape the outputs from language models according to pre-defined schemas.  When used alongside prompt templates, it enables more structured interactions with language models, making it easier to extract and work with the information provided by the model.\n",
        "\n",
        "The prompt template includes the format instructions from our parser, which guide the language model to produce the output in the desired structured format. The idea is to demonstrate how you could use **PydanticOutputParser** class to receive the output as a type List that holds each bullet point instead of a string. The advantage of having a list is the possibility to loop through the results or index a specific item.\n",
        "\n",
        "As mentioned before, the **PydanticOutputParser wrapper** is used to create a parser that will parse the output from the string into a data structure. The custom **ArticleSummary** class, which inherits the Pydantic package’s  **BaseModel class,** will be used to parse the model’s output.\n",
        "\n",
        "We defined the schema to present a **title** along with a **summary variabl**e that represents a **list of strings** using the **Field object**. The **description argument** will describe what each variable must represent and help the model to achieve it. Our custom class also includes a **validator function** to ensure that the generated output contains at least three bullet points"
      ],
      "metadata": {
        "id": "rvNZ2gaM3u82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import field_validator\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "#create output parser\n",
        "class ArticleSumamry(BaseModel):\n",
        "  title: str=Field(description = \"Title of the article\")\n",
        "  summary : List[str]= Field(description= \"Bulleted list summary of the article\")\n",
        "\n",
        "# validating whether the generated summary has at least three lines\n",
        "@field_validator(\"summary\")\n",
        "\n",
        "def has_three_or_more_lines(cls, list_of_lines):\n",
        "        if len(list_of_lines) < 3:\n",
        "            raise ValueError(\"Generated summary has less than three bullet points!\")\n",
        "        return list_of_lines\n",
        "\n",
        "#setup output parser\n",
        "parser = PydanticOutputParser(pydantic_object=ArticleSumamry)"
      ],
      "metadata": {
        "id": "Ts_aAu5L3wsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "# create prompt template\n",
        "# notice that we are specifying the \"partial_variables\" parameter\n",
        "template = \"\"\"\n",
        "You are a very good assistant that summarizes online articles.\n",
        "\n",
        "Here's the article you want to summarize.\n",
        "\n",
        "==================\n",
        "Title: {article_title}\n",
        "\n",
        "{article_text}\n",
        "==================\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"article_title\", \"article_text\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "# Format the prompt using the article title and text obtained from scraping\n",
        "formatted_prompt = prompt.format_prompt(article_title=article_title, article_text=article_text)"
      ],
      "metadata": {
        "id": "COltAjdIA1hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, **the GPT-3 model with the temperature set to 0.0 ** is initialized, which means the output will be deterministic, favoring the most likely outcome over randomness/creativity. The parser object then converts the string output from the model to a defined schema using the .parse() method."
      ],
      "metadata": {
        "id": "dN8_8-tAA69D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "# instantiate model class\n",
        "model = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)\n",
        "\n",
        "# Use the model to generate a summary\n",
        "output = model(formatted_prompt.to_string())\n",
        "\n",
        "# Parse the output into the Pydantic model\n",
        "parsed_output = parser.parse(output.split(\"\\\"]}\")[0] + \"\\\"]}\")\n",
        "parsed_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSiyFn9ZA9bI",
        "outputId": "8e8206c9-7bd0-4ccb-b431-f552f4401d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ArticleSumamry(title='Meta claims its new AI supercomputer will set records', summary=[\"Meta (formerly Facebook) has unveiled an AI supercomputer called the AI Research SuperCluster (RSC) that is set to be the world's fastest once completed in mid-2022.\", \"The RSC is already being used by Meta's researchers for training large NLP and computer vision models.\", 'Meta hopes that the RSC will pave the way for building technologies for the metaverse and will be 20x faster than their current V100-based clusters.', 'The RSC is also estimated to be 9x faster at running the NVIDIA Collective Communication Library (NCCL) and 3x faster at training large-scale NLP workflows.', \"Meta's previous AI research infrastructure only used open source and publicly-available datasets, but the RSC was designed with security and privacy controls in mind to allow for the use of real-world data from their production systems.\", 'This will enable Meta to advance research for tasks such as identifying harmful content on their platforms.', 'The RSC is expected to be the first AI supercomputer to tackle performance, reliability, security, and privacy at such a large'])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Pydantic output parser is a powerful method for molding and structuring the output from language models. It uses the Pydantic library, known for its data validation capabilities, to define and enforce data schemas for the model's output.\n",
        "\n",
        "# **This is a recap of what we did:**"
      ],
      "metadata": {
        "id": "qET6lz-hBUOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   We defined a **Pydantic data structure** named **ArticleSummary.** This model serves as a blueprint for the desired structure of the generated article summary. It comprises fields for the title and the summary, which is expected to be a list of strings representing bullet points.\n",
        "*  Importantly, we incorporate a **validator** within this model to ensure the summary comprises at least three points, thereby maintaining a certain level of detail in the summarization.\n",
        "*  We then instantiate a **parser object** using our ArticleSummary class. This parser plays a crucial role in ensuring the output generated by the language model aligns with the **defined structures** of our custom schema.\n",
        "*  To direct the language model's output, we create the **prompt template**. The template instructs the model to act as an assistant that summarizes online articles by incorporating the parser object.\n",
        "*  So, output parsers enable us to specify the desired format of the model's output, making extracting meaningful information from the model's responses easier."
      ],
      "metadata": {
        "id": "0D2WdURND9lZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "We've successfully navigated the path of crafting our ***News Articles Summarizer*** leveraging the **potential of PromptTemplates and OutputParsers,** showing the capabilities of prompt handling LangChain. The Pydantic output parser is a powerful method for molding and structuring the output from language models. It uses the Pydantic library, known for its data validation capabilities, to define and enforce data schemas for the model's output.\n",
        "\n",
        "Following this, we define a Pydantic model named \"ArticleSummary.” This model serves as a blueprint for the desired structure of the generated article summary. It comprises fields for the title and the summary, which is expected to be a list of strings representing bullet points. Importantly, we incorporate a validator within this model to ensure the summary comprises at least three points, thereby maintaining a certain level of detail in the summarization.\n",
        "\n",
        "We then instantiate a PydanticOutputParser, passing it to the \"ArticleSummary\" model. This parser plays a crucial role in ensuring the output generated by the language model aligns with the structure outlined in the \"Article Summary\" model.\n",
        "\n",
        "A good understanding of prompt and output design nuances equips you to customize the model to produce results that perfectly match your specific requirements."
      ],
      "metadata": {
        "id": "ftBTSIIUEusl"
      }
    }
  ]
}